{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d607a58",
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import math \n",
    "import random\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "import MDAnalysis as mda\n",
    "from MDAnalysis.analysis import dihedrals \n",
    "import nglview as nv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import json\n",
    "from datetime import datetime\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1402805",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def set_all_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca935574",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class MyArgs(object):\n",
    "\n",
    "    def __init__(self, config_filename='params.cfg'):\n",
    "\n",
    "        config = configparser.ConfigParser()\n",
    "        config.read(config_filename)\n",
    "\n",
    "        self.pdb_filename = config['System']['pdb_filename']\n",
    "        self.traj_dcd_filename = config['System']['traj_dcd_filename']\n",
    "        self.sys_name = config['System']['sys_name']\n",
    "          \n",
    "        #set training parameters\n",
    "        self.use_gpu =config['Training'].getboolean('use_gpu')\n",
    "        self.batch_size = config['Training'].getint('batch_size')\n",
    "        self.num_epochs = config['Training'].getint('num_epochs')\n",
    "        self.test_ratio = config['Training'].getfloat('test_ratio')\n",
    "        self.learning_rate = config['Training'].getfloat('learning_rate')\n",
    "        self.optimizer = config['Training']['optimizer']\n",
    "        self.k = config['Training'].getint('encoded_dim')\n",
    "        self.e_layer_dims = [int(x) for x in config['Training']['encoder_hidden_layer_dims'].split(',')]\n",
    "        self.d_layer_dims = [int(x) for x in config['Training']['decoder_hidden_layer_dims'].split(',')]\n",
    "        self.load_model_filename =  config['Training']['load_model_filename']\n",
    "        self.model_save_dir = config['Training']['model_save_dir'] \n",
    "        self.save_model_every_step = config['Training'].getint('save_model_every_step')\n",
    "        \n",
    "        self.activation_name = config['Training']['activation'] \n",
    "        self.activation = getattr(torch.nn, self.activation_name) \n",
    "\n",
    "        self.align_selector = config['Training']['align_mda_selector']\n",
    "        self.train_atom_selector = config['Training']['train_mda_selector'] \n",
    "        self.seed = config['Training'].getint('seed')\n",
    "\n",
    "        if self.seed:\n",
    "            set_all_seeds(self.seed)\n",
    "\n",
    "        # encoded dimension\n",
    "        # CUDA support\n",
    "        if torch.cuda.is_available() and self.use_gpu:\n",
    "            self.device = torch.device('cuda')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "\n",
    "        print (f'Parameters loaded from: {config_filename}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9629f51a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class Trajectory(object):\n",
    "    def __init__(self, args):\n",
    "        # load the trajectory data from DCD file\n",
    "        self.u = mda.Universe(args.pdb_filename, args.traj_dcd_filename)\n",
    "\n",
    "        # load the reference configuration from the PDB file\n",
    "        self.ref = mda.Universe(args.pdb_filename) \n",
    "\n",
    "        self.atoms_info = pd.DataFrame(\n",
    "            np.array([self.ref.atoms.ids, self.ref.atoms.names,\n",
    "                self.ref.atoms.types, self.ref.atoms.masses,\n",
    "                self.ref.atoms.resids, self.ref.atoms.resnames]).T, \n",
    "            columns=['id', 'name', 'type', 'mass', 'resid', 'resname']\n",
    "            )\n",
    "\n",
    "        # print information of trajectory\n",
    "        print ('\\nMD system:\\n\\\n",
    "        \\tno. of atoms: {}\\n\\\n",
    "        \\tno. of residues: {}\\n'.format(self.ref.trajectory.n_atoms, self.ref.residues.n_residues)\n",
    "              )\n",
    "        print ('Detailed atom information:\\n', self.atoms_info)\n",
    "\n",
    "        print ('\\nSummary:\\n', self.atoms_info['type'].value_counts().rename_axis('type').reset_index(name='counts'))\n",
    "\n",
    "        self.load_traj()\n",
    "\n",
    "        self.ref_pos = self.ref.atoms.positions\n",
    "\n",
    "    def load_traj(self):\n",
    "\n",
    "        print ('\\nloading trajectory to numpy array...', end='')\n",
    "        # load trajectory to torch tensor \n",
    "        self.trajectory = torch.from_numpy(self.u.trajectory.timeseries(order='fac')).double()\n",
    "\n",
    "        print ('done.')\n",
    "\n",
    "        # print information of trajectory\n",
    "        print ('\\nTrajectory Info:\\n\\\n",
    "        \\tno. of frames in trajectory data: {}\\n\\\n",
    "        \\ttimestep: {:.1f}ps\\n\\\n",
    "        \\ttime length: {:.1f}ps\\n\\\n",
    "        \\tshape of trajectory data array: {}\\n'.format(self.u.trajectory.n_frames, \n",
    "                                          self.u.trajectory.time, \n",
    "                                          self.u.trajectory.totaltime,\n",
    "                                          self.trajectory.shape\n",
    "                                         )\n",
    "              )\n",
    "        self.weights = np.ones(self.trajectory.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4966333",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, args, traj_obj):\n",
    "\n",
    "        super(Preprocessing, self).__init__()\n",
    "\n",
    "        self.align_atom_ids = traj_obj.u.select_atoms(args.align_selector).ids\n",
    "        self.train_atom_ids = traj_obj.u.select_atoms(args.train_atom_selector).ids \n",
    "\n",
    "        print ('\\nInfor of preprocess layer.\\naligning by atoms:')\n",
    "        print (traj_obj.atoms_info.loc[traj_obj.atoms_info['id'].isin(self.align_atom_ids)][['id','name', 'type']])\n",
    "\n",
    "        self.align_atom_indices = torch.tensor(self.align_atom_ids-1).long() # minus one, such that the index starts from 0\n",
    "        self.train_atom_indices = torch.tensor(self.train_atom_ids-1).long() \n",
    "\n",
    "        self.ref_x = torch.from_numpy(traj_obj.ref_pos[self.align_atom_indices, :]).double()        \n",
    "\n",
    "        # shift reference state \n",
    "        ref_c = torch.mean(self.ref_x, 0) \n",
    "        self.ref_x = self.ref_x - ref_c\n",
    "        \n",
    "    def show_info(self):\n",
    "        print ('atom indices used for alignment: ', self.align_atom_indices.numpy())\n",
    "        print ('atom indices used for input layer: ', self.train_atom_indices.numpy())\n",
    "        print ('\\n\\treference state used in aligment:\\n', self.ref_x.numpy())\n",
    "\n",
    "    def align(self, traj):  \n",
    "        \"\"\"\n",
    "        align trajectory by translation and rotation\n",
    "        \"\"\"\n",
    "                         \n",
    "        traj_selected_atoms = traj[:, self.align_atom_indices, :]\n",
    "        # centers\n",
    "        x_c = torch.mean(traj_selected_atoms, 1, True)\n",
    "        # translation\n",
    "        x_notran = traj_selected_atoms - x_c \n",
    "        \n",
    "        xtmp = x_notran.permute((0,2,1))\n",
    "        prod = torch.matmul(xtmp, self.ref_x) # dimension: traj_length x 3 x 3\n",
    "        u, s, vh = torch.linalg.svd(prod)\n",
    "\n",
    "        diag_mat = torch.diag(torch.ones(3)).double().unsqueeze(0).repeat(traj.size(0), 1, 1)\n",
    "\n",
    "        sign_vec = torch.sign(torch.linalg.det(torch.matmul(u, vh))).detach()\n",
    "        diag_mat[:,2,2] = sign_vec\n",
    "\n",
    "        rotate_mat = torch.bmm(torch.bmm(u, diag_mat), vh)\n",
    "\n",
    "        aligned_traj = torch.matmul(traj-x_c, rotate_mat) \n",
    "                \n",
    "        return aligned_traj     \n",
    "    \n",
    "    def forward(self, inp):\n",
    "        inp = self.align(inp)\n",
    "        inp = torch.flatten(inp[:,self.train_atom_indices,:], start_dim=1)\n",
    "        return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312b69a2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ColVar(torch.nn.Module):\n",
    "    def __init__(self, preprocessing_layer, encoder):\n",
    "        super(ColVar, self).__init__()\n",
    "        self.preprocessing_layer = preprocessing_layer\n",
    "        self.encoder = encoder\n",
    "    def forward(self, inp):\n",
    "        return self.encoder(self.preprocessing_layer(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0869cdbf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#Auto encoders class and functions for training.\n",
    "def create_sequential_nn(layer_dims, activation=torch.nn.Tanh()):\n",
    "    layers = []\n",
    "    for i in range(len(layer_dims)-2) :\n",
    "        layers.append(torch.nn.Linear(layer_dims[i], layer_dims[i+1])) \n",
    "        layers.append(activation)\n",
    "    layers.append(torch.nn.Linear(layer_dims[-2], layer_dims[-1])) \n",
    "\n",
    "    return torch.nn.Sequential(*layers).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654153b8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(torch.nn.Module):\n",
    "    def __init__(self, e_layer_dims, d_layer_dims, activation=torch.nn.Tanh()):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = create_sequential_nn(e_layer_dims, activation)\n",
    "        self.decoder = create_sequential_nn(d_layer_dims, activation)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        return self.decoder(self.encoder(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb312da",
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class TrainingTask(object):\n",
    "    def __init__(self, args, traj_obj, preprocessing_layer, ae_model):\n",
    "\n",
    "        self.ae_model = ae_model\n",
    "        self.learning_rate = args.learning_rate\n",
    "        self.preprocessing_layer = preprocessing_layer\n",
    "        self.num_epochs= args.num_epochs\n",
    "        self.batch_size = args.batch_size \n",
    "        self.test_ratio = args.test_ratio\n",
    "        self.save_model_every_step = args.save_model_every_step\n",
    "\n",
    "        if os.path.isfile(args.load_model_filename): \n",
    "            self.ae_model.load_state_dict(torch.load(args.load_model_filename))\n",
    "            print (f'model parameters loaded from: {args.load_model_filename}')\n",
    "\n",
    "        if args.optimizer == 'Adam':\n",
    "            self.optimizer = torch.optim.Adam(self.ae_model.parameters(), lr=self.learning_rate)\n",
    "        else:\n",
    "            self.optimizer = torch.optim.SGD(self.ae_model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        # path to store log data\n",
    "        prefix = f\"{args.sys_name}-\" \n",
    "        self.model_path = os.path.join(args.model_save_dir, prefix + time.strftime(\"%Y-%m-%d-%H:%M:%S\", time.localtime()))\n",
    "        print ('\\nLog directory: {}\\n'.format(self.model_path))\n",
    "        self.writer = SummaryWriter(self.model_path)\n",
    "\n",
    "        self.traj = self.preprocessing_layer(traj_obj.trajectory)\n",
    "\n",
    "        self.traj_weights = traj_obj.weights\n",
    "\n",
    "        # print information of trajectory\n",
    "        print ('\\nshape of preprocessed trajectory data array:\\n {}'.format(self.traj.shape))\n",
    "         \n",
    "    def save_model(self):\n",
    "        #save the model\n",
    "        trained_model_filename = f'{self.model_path}/trained_model.pt'\n",
    "        torch.save(self.ae_model.state_dict(), trained_model_filename)  \n",
    "        print (f'trained model is saved at:\\n\\t{trained_model_filename}\\n')\n",
    "\n",
    "        cv = ColVar(self.preprocessing_layer, self.ae_model.encoder)\n",
    "\n",
    "        trained_cv_script_filename = f'{self.model_path}/trained_cv_scripted.pt'\n",
    "        torch.jit.script(cv).save(trained_cv_script_filename)\n",
    "\n",
    "        print (f'script model for CVs is saved at:\\n\\t{trained_cv_script_filename}\\n')\n",
    "\n",
    "# Next, we define the training function \n",
    "    def train(self):\n",
    "        \"\"\"Function to train an AE model\n",
    "        \"\"\"\n",
    "        #--- prepare the data ---\n",
    "        # split the dataset into a training set (and its associated weights) and a test set\n",
    "        X_train, X_test, w_train, w_test = train_test_split(self.traj, self.traj_weights, test_size=self.test_ratio)  \n",
    "        # intialization of the methods to sample with replacement from the data points (needed since weights are present)\n",
    "        train_sampler = torch.utils.data.WeightedRandomSampler(w_train, len(w_train))\n",
    "        test_sampler  = torch.utils.data.WeightedRandomSampler(w_test, len(w_test))\n",
    "        # method to construct data batches and iterate over them\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=X_train,\n",
    "                                                   batch_size=self.batch_size,\n",
    "                                                   shuffle=False,\n",
    "                                                   sampler=train_sampler)\n",
    "        test_loader  = torch.utils.data.DataLoader(dataset=X_test,\n",
    "                                                   batch_size=self.batch_size,\n",
    "                                                   shuffle=False,\n",
    "                                                   sampler=test_sampler)\n",
    "        \n",
    "        loss_func = torch.nn.MSELoss()\n",
    "        # --- start the training over the required number of epochs ---\n",
    "        self.loss_list = []\n",
    "        print (\"\\ntraining starts, %d epochs in total.\" % self.num_epochs) \n",
    "        for epoch in tqdm(range(self.num_epochs)):\n",
    "            # Train the model by going through the whole dataset\n",
    "            self.ae_model.train()\n",
    "            train_loss = []\n",
    "            for iteration, X in enumerate(train_loader):\n",
    "                # Clear gradients w.r.t. parameters\n",
    "                self.optimizer.zero_grad()\n",
    "                # Forward pass to get output\n",
    "                out = self.ae_model(X)\n",
    "                # Evaluate loss\n",
    "                loss = loss_func(out, X)\n",
    "                # Get gradient with respect to parameters of the model\n",
    "                loss.backward()\n",
    "                # Store loss\n",
    "                train_loss.append(loss)\n",
    "                # Updating parameters\n",
    "                self.optimizer.step()\n",
    "            # Evaluate the test loss on the test dataset\n",
    "            self.ae_model.eval()\n",
    "            with torch.no_grad():\n",
    "                # Evaluation of test loss\n",
    "                test_loss = []\n",
    "                for iteration, X in enumerate(test_loader):\n",
    "                    out = self.ae_model(X)\n",
    "                    # Evaluate loss\n",
    "                    loss = loss_func(out,X)\n",
    "                    # Store loss\n",
    "                    test_loss.append(loss)\n",
    "                self.loss_list.append([torch.tensor(train_loss), torch.tensor(test_loss)])\n",
    "                \n",
    "            self.writer.add_scalar('Loss/train', torch.mean(torch.tensor(train_loss)), epoch)\n",
    "            self.writer.add_scalar('Loss/test', torch.mean(torch.tensor(test_loss)), epoch)\n",
    "\n",
    "            if epoch % self.save_model_every_step == 0 :\n",
    "                self.save_model()\n",
    "\n",
    "        print (\"training ends.\\n\") \n",
    "\n",
    "    def output_loss(self):\n",
    "        loss_evol1 = []\n",
    "        for i in range(len(self.loss_list)):\n",
    "            loss_evol1.append([torch.mean(self.loss_list[i][0]), torch.mean(self.loss_list[i][1])])\n",
    "        loss_evol1 = np.array(loss_evol1)\n",
    "\n",
    "        start_epoch_index = 1\n",
    "        ax  = plt.axes() \n",
    "        ax.plot(range(start_epoch_index, self.num_epochs), loss_evol1[start_epoch_index:, 0], '--', label='train loss', marker='o')\n",
    "        ax.plot(range(1, num_epochs), loss_evol1[start_epoch_index:, 1], '-.', label='test loss', marker='+')\n",
    "        ax.legend()\n",
    "        ax.set_title('losses')\n",
    "\n",
    "        fig_filename = 'training_loss_%s.jpg' % pot_name\n",
    "        fig.savefig(fig_filename)\n",
    "        print ('training loss plotted to file: %s' % fig_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47f2848",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    args = MyArgs()\n",
    "\n",
    "    traj_obj = Trajectory(args)\n",
    "\n",
    "    #preprocessing the trajectory data\n",
    "    preprocessing_layer = Preprocessing(args, traj_obj)\n",
    "\n",
    "    input_dim = 3 * len(preprocessing_layer.train_atom_ids)\n",
    "    e_layer_dims = [input_dim] + args.e_layer_dims + [args.k]\n",
    "    d_layer_dims = [args.k] + args.d_layer_dims + [input_dim]\n",
    "\n",
    "    ae_model = AutoEncoder(e_layer_dims, d_layer_dims, args.activation())\n",
    "\n",
    "    print ('\\nAutoencoder:\\n', ae_model)\n",
    "    # encoded dimension\n",
    "    print ('\\nInput dim: {},\\tencoded dim: {}\\n'.format(input_dim, args.k))\n",
    "    #input dimension of nn\n",
    "    print ('\\n{} Atoms used in define neural network:\\n'.format(len(preprocessing_layer.train_atom_ids)), \\\n",
    "                    traj_obj.atoms_info.loc[traj_obj.atoms_info['id'].isin(preprocessing_layer.train_atom_ids)][['id','name', 'type']])\n",
    "\n",
    "    train_obj = TrainingTask(args, traj_obj, preprocessing_layer, ae_model)\n",
    "    train_obj.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf428035",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python",
   "formats": "ipynb,auto:light",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
