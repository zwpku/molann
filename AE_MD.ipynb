{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d607a58",
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import integrate\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import math \n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split as ttsplit\n",
    "\n",
    "import MDAnalysis as mda\n",
    "from MDAnalysis.analysis import dihedrals, rms\n",
    "import nglview as nv\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384f43f1",
   "metadata": {},
   "source": [
    "## Part 1: load MD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9629f51a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mda' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11745/828801982.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# load the trajectory data from DCD file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUniverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdb_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraj_dcd_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m# load the reference configuration from the PDB file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUniverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdb_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mda' is not defined"
     ]
    }
   ],
   "source": [
    "sysname = 'AlanineDipeptide'\n",
    "# name of PDB file\n",
    "pdb_filename = \"./AlanineDipeptideOpenMM/vacuum.pdb\"\n",
    "# name of DCD file\n",
    "#output_path = './allegro-data/working_dir/Langevin_working_dir' \n",
    "output_path = './allegro-data/working_dir/Langevin_working_dir-test3-plumed/' \n",
    "traj_dcd_filename = '%s/traj.dcd' % output_path\n",
    "\n",
    "# load the trajectory data from DCD file\n",
    "u = mda.Universe(pdb_filename, traj_dcd_filename)\n",
    "# load the reference configuration from the PDB file\n",
    "ref = mda.Universe(pdb_filename) \n",
    "\n",
    "# print some information\n",
    "print ('residues: ', u.residues)\n",
    "print ('trajectory: ', u.trajectory)\n",
    "print ('reference: ', ref.trajectory)\n",
    "\n",
    "# display the trajectory\n",
    "view = nv.show_mdanalysis(u)\n",
    "print ('number of frames: %d ' % view.max_frame)\n",
    "view    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d8c649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the Ramachandran plot of two dihedral angles\n",
    "ax = plt.gca()\n",
    "r = dihedrals.Ramachandran(u.select_atoms('resid 2')).run()\n",
    "r.plot(ax, color='black', marker='.') #, ref=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12091344",
   "metadata": {},
   "source": [
    "## Part 2: define neural network model and training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654153b8",
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#We now define the Auto encoders classes and useful functions for the training.\n",
    "\n",
    "class DeepAutoEncoder(nn.Module):\n",
    "    def __init__(self, encoder_dims, decoder_dims):\n",
    "        \"\"\"Initialise auto encoder with hyperbolic tangent activation function\n",
    "\n",
    "        :param encoder_dims: list, List of dimensions for encoder, including input/output layers\n",
    "        :param decoder_dims: list, List of dimensions for decoder, including input/output layers\n",
    "        \"\"\"\n",
    "        super(DeepAutoEncoder, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(encoder_dims)-2) :\n",
    "            layers.append(torch.nn.Linear(encoder_dims[i], encoder_dims[i+1])) \n",
    "            layers.append( torch.nn.Tanh() )\n",
    "        layers.append(torch.nn.Linear(encoder_dims[-2], encoder_dims[-1])) \n",
    "\n",
    "        self.encoder = torch.nn.Sequential(*layers)\n",
    "\n",
    "        layers = []\n",
    "        for i in range(len(decoder_dims)-2) :\n",
    "            layers.append(torch.nn.Linear(decoder_dims[i], decoder_dims[i+1])) \n",
    "            layers.append( torch.nn.Tanh() )\n",
    "        layers.append(torch.nn.Linear(decoder_dims[-2], decoder_dims[-1])) \n",
    "\n",
    "        self.decoder = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        encoded = self.encoder(inp)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "def xi_ae(model,  x):\n",
    "    \"\"\"Collective variable defined through an auto encoder model\n",
    "\n",
    "    :param model: Neural network model build with PyTorch\n",
    "    :param x: np.array, position, ndim = 2, shape = (1,1)\n",
    "\n",
    "    :return: xi: np.array\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    if torch.is_tensor(x) == False :\n",
    "        x = torch.from_numpy(x).float()\n",
    "    return model.encoder(x).detach().numpy()\n",
    "\n",
    "\n",
    "# Next, we define the training function \n",
    "def train(model, optimizer, traj, weights, num_epochs=10, batch_size=32, test_size=0.2):\n",
    "    \"\"\"Function to train an AE model\n",
    "    \n",
    "    :param model: Neural network model built with PyTorch,\n",
    "    :param loss_function: Function built with PyTorch tensors or built-in PyTorch loss function\n",
    "    :param optimizer: PyTorch optimizer object\n",
    "    :param traj: np.array, physical trajectory (in the potential pot), ndim == 2, shape == T // save + 1, pot.dim\n",
    "    :param weights: np.array, weights of each point of the trajectory when the dynamics is biased, ndim == 1, shape == T // save + 1, 1\n",
    "    :param num_epochs: int, number of times the training goes through the whole dataset\n",
    "    :param batch_size: int, number of data points per batch for estimation of the gradient\n",
    "    :param test_size: float, between 0 and 1, giving the proportion of points used to compute test loss\n",
    "\n",
    "    :return: model, trained neural net model\n",
    "    :return: loss_list, list of lists of train losses and test losses; one per batch per epoch\n",
    "    \"\"\"\n",
    "    #--- prepare the data ---\n",
    "    # split the dataset into a training set (and its associated weights) and a test set\n",
    "    X_train, X_test, w_train, w_test = ttsplit(traj, weights, test_size=test_size)\n",
    "    X_train = torch.tensor(X_train.astype('float32'))\n",
    "    X_test = torch.tensor(X_test.astype('float32'))\n",
    "    w_train = torch.tensor(w_train.astype('float32'))\n",
    "    w_test = torch.tensor(w_test.astype('float32'))\n",
    "    # intialization of the methods to sample with replacement from the data points (needed since weights are present)\n",
    "    train_sampler = torch.utils.data.WeightedRandomSampler(w_train, len(w_train))\n",
    "    test_sampler  = torch.utils.data.WeightedRandomSampler(w_test, len(w_test))\n",
    "    # method to construct data batches and iterate over them\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=X_train,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False,\n",
    "                                               sampler=train_sampler)\n",
    "    test_loader  = torch.utils.data.DataLoader(dataset=X_test,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False,\n",
    "                                               sampler=test_sampler)\n",
    "    \n",
    "    # --- start the training over the required number of epochs ---\n",
    "    loss_list = []\n",
    "    print (\"\\ntraining starts, %d epochs in total.\" % num_epochs) \n",
    "    for epoch in range(num_epochs):\n",
    "        # Train the model by going through the whole dataset\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for iteration, X in enumerate(train_loader):\n",
    "            # Set gradient calculation capabilities\n",
    "            X.requires_grad_()\n",
    "            # Clear gradients w.r.t. parameters\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass to get output\n",
    "            out = model(X)\n",
    "            # Evaluate loss\n",
    "            loss = nn.MSELoss(out, X)\n",
    "            # Get gradient with respect to parameters of the model\n",
    "            loss.backward()\n",
    "            # Store loss\n",
    "            train_loss.append(loss)\n",
    "            # Updating parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "        # Evaluate the test loss on the test dataset\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Evaluation of test loss\n",
    "            test_loss = []\n",
    "            for iteration, X in enumerate(test_loader):\n",
    "                out = model(X)\n",
    "                # Evaluate loss\n",
    "                loss = nn.MSELoss(out, X)\n",
    "                # Store loss\n",
    "                test_loss.append(loss)\n",
    "            loss_list.append([torch.tensor(train_loss), torch.tensor(test_loss)])\n",
    "\n",
    "    print (\"training ends.\\n\") \n",
    "    return model, loss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cac50a",
   "metadata": {},
   "source": [
    "## Part 3: train autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ad270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All the parameters are set in the cell below. \n",
    "seed = None \n",
    "# for training\n",
    "batch_size = 10000\n",
    "num_epochs = 500\n",
    "learning_rate = 0.005\n",
    "n_bins_z = 20          # number of bins in the encoded dimension\n",
    "optimizer_algo='Adam'  # Adam by default, otherwise SGD\n",
    "#dimensions\n",
    "ae1 = DeepAutoEncoder([2, 20, 20, 1], [1, 20, 20, 2]) \n",
    "print(\"test using NN:\", ae1) \n",
    "save_fig_to_file = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80852277",
   "metadata": {},
   "source": [
    "Training the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3751a7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "if optimizer_algo == 'Adam':\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "else:\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "(\n",
    "    ae1,\n",
    "    loss_list \n",
    ") = train(ae1, \n",
    "          optimizer, \n",
    "          trajectory, \n",
    "          np.ones(trajectory.shape[0]), \n",
    "          batch_size=batch_size, \n",
    "          num_epochs=num_epochs\n",
    "          )\n",
    "\n",
    "#--- Compute average train per epoch ---\n",
    "loss_evol1 = []\n",
    "for i in range(len(loss_list)):\n",
    "    loss_evol1.append([torch.mean(loss_list[i][0]), torch.mean(loss_list[i][1])])\n",
    "loss_evol1 = np.array(loss_evol1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6654229",
   "metadata": {},
   "source": [
    "Plot the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24567d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch_index = 1\n",
    "fig, (ax0, ax1, ax2)  = plt.subplots(1,3, figsize=(12,4)) \n",
    "ax0.plot(range(start_epoch_index, num_epochs), loss_evol1[start_epoch_index:, 0], '--', label='train loss', marker='o')\n",
    "ax0.plot(range(1, num_epochs), loss_evol1[start_epoch_index:, 1], '-.', label='test loss', marker='+')\n",
    "ax0.legend()\n",
    "ax0.set_title('losses')\n",
    "\n",
    "if save_fig_to_file :\n",
    "    fig_filename = 'training_loss_%s.jpg' % pot_name\n",
    "    fig.savefig(fig_filename)\n",
    "    print ('training loss plotted to file: %s' % fig_filename)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python",
   "formats": "ipynb,auto:light",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
