{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d607a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import math \n",
    "import random\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "import MDAnalysis as mda\n",
    "from MDAnalysis.analysis import dihedrals, rms, align\n",
    "import nglview as nv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384f43f1",
   "metadata": {},
   "source": [
    "## Part 1: prepare MD data\n",
    "#### 1.1. show some information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9629f51a",
   "metadata": {
    "lines_to_end_of_cell_marker": 2,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "sys_name = 'AlanineDipeptide'\n",
    "\n",
    "# name of PDB file\n",
    "pdb_filename = \"MD_samplers/AlanineDipeptideOpenMM/vacuum.pdb\"\n",
    "# name of DCD file\n",
    "output_path = 'MD_samplers/allegro-data/working_dir/Langevin_working_dir' \n",
    "#output_path = './allegro-data/working_dir/Langevin_working_dir-test3-plumed/' \n",
    "traj_dcd_filename = '%s/traj.dcd' % output_path\n",
    "\n",
    "# load the reference configuration from the PDB file\n",
    "ref = mda.Universe(pdb_filename) \n",
    "\n",
    "atoms_info = pd.DataFrame(\n",
    "    np.array([ref.atoms.ids, ref.atoms.names, ref.atoms.types, ref.atoms.masses, ref.atoms.resids, ref.atoms.resnames]).T, \n",
    "    columns=['id', 'name', 'type', 'mass', 'resid', 'resname']\n",
    ")\n",
    "\n",
    "# print information of trajectory\n",
    "print ('\\nMD system:\\n\\\n",
    "\\tno. of atoms: {}\\n\\\n",
    "\\tno. of residues: {}\\n'.format(ref.trajectory.n_atoms, ref.residues.n_residues)\n",
    "      )\n",
    "print ('Detailed atom information:\\n', atoms_info)\n",
    "\n",
    "print ('\\nSummary:\\n', atoms_info['type'].value_counts().rename_axis('type').reset_index(name='counts'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fa901f",
   "metadata": {},
   "source": [
    "#### 1.2 load trajectory, and align with respect to refenrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4966333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a class for aligning trajectory\n",
    "class Align(object):\n",
    "    def __init__(self, ref_pos, align_atom_indices):\n",
    "        self.atom_indices = align_atom_indices\n",
    "        self.ref_x = torch.from_numpy(ref_pos[align_atom_indices]).double()        \n",
    "        # shift reference state \n",
    "        ref_c = torch.mean(self.ref_x, 0) \n",
    "        self.ref_x = self.ref_x - ref_c\n",
    "        self.num_atoms = len(align_atom_indices) * 3\n",
    "        \n",
    "    def show_info(self):\n",
    "        print (f'\\nAlignment:\\n indices of {len(self.atom_indices)} Atoms used in alignment:\\n\\t', self.atom_indices)\n",
    "        print ('\\n\\treference state used in aligment:\\n', self.ref_x.numpy())\n",
    "        \n",
    "    def dist_to_ref(self, traj):\n",
    "        return torch.linalg.norm(torch.sub(traj[:,self.atom_indices,:], self.ref_x), dim=(1,2)).numpy()     \n",
    "            \n",
    "    def __call__(self, traj):  \n",
    "        \"\"\"\n",
    "        align trajectory by translation and rotation\n",
    "        \"\"\"\n",
    "        traj_selected_atoms = traj[:, self.atom_indices, :]\n",
    "        # centers\n",
    "        x_c = torch.mean(traj_selected_atoms, 1, True)\n",
    "        # translation\n",
    "        x_notran = traj_selected_atoms - x_c \n",
    "        \n",
    "        xtmp = x_notran.permute((0,2,1))\n",
    "        prod = torch.matmul(xtmp, self.ref_x) # dimension: traj_length x 3 x 3\n",
    "        u, s, vh = torch.linalg.svd(prod)\n",
    "\n",
    "        diag_mat = torch.diag(torch.ones(3)).double().unsqueeze(0).repeat(traj.size(0), 1, 1)\n",
    "\n",
    "        sign_vec = torch.sign(torch.linalg.det(torch.matmul(u, vh))).detach()\n",
    "        diag_mat[:,2,2] = sign_vec\n",
    "\n",
    "        rotate_mat = torch.bmm(torch.bmm(u, diag_mat), vh)\n",
    "\n",
    "        return torch.matmul(traj-x_c, rotate_mat)     \n",
    "\n",
    "# load the trajectory data from DCD file\n",
    "u = mda.Universe(pdb_filename, traj_dcd_filename)\n",
    "\n",
    "print ('\\n[Task 1/2] load trajectory to numpy array...', end='')\n",
    "# load trajectory to torch tensor \n",
    "trajectory = torch.from_numpy(u.trajectory.timeseries(order='fac')).double()\n",
    "print ('done.')\n",
    "\n",
    "# print information of trajectory\n",
    "print ('\\nTrajectory Info:\\n\\\n",
    "\\tno. of frames in trajectory data: {}\\n\\\n",
    "\\ttimestep: {:.1f}ps\\n\\\n",
    "\\ttime length: {:.1f}ps\\n\\\n",
    "\\tshape of trajectory data array: {}'.format(u.trajectory.n_frames, \n",
    "                                  u.trajectory.time, \n",
    "                                  u.trajectory.totaltime,\n",
    "                                  trajectory.shape\n",
    "                                 )\n",
    "      )\n",
    "\n",
    "align_selector = \"type C or type O or type N\"\n",
    "selected_ids = u.select_atoms(align_selector).ids\n",
    "\n",
    "align_functor = Align(ref.atoms.positions, selected_ids-1)\n",
    "\n",
    "print ('\\n[Task 2/2] aligning by atoms:')\n",
    "print (atoms_info.loc[atoms_info['id'].isin(selected_ids)][['id','name', 'type']])\n",
    "\n",
    "head_frames = 5\n",
    "dist_list = align_functor.dist_to_ref(trajectory[:head_frames,:,:])\n",
    "trajectory = align_functor(trajectory)\n",
    "dist_list_aligned = align_functor.dist_to_ref(trajectory[:head_frames,:,:])\n",
    "\n",
    "\"\"\"\n",
    "#One could also use the alignment methods provided in MDAnalysis package \n",
    "rmsd_list = []\n",
    "for idx in range(head_frames):\n",
    "    rmsd_ret = rms.rmsd(trajectory[idx,selected_ids-1,:], ref_pos[selected_ids-1,:], superposition=False)\n",
    "    rmsd_list.append(rmsd_ret)\n",
    "\n",
    "align.AlignTraj(u,  # trajectory to align\n",
    "                ref,  # reference\n",
    "                select=align_selector,  # selection of atoms to align\n",
    "                filename=None,  # file to write the trajectory to\n",
    "                in_memory=True,\n",
    "                match_atoms=True,  # whether to match atoms based on mass\n",
    "               ).run()\n",
    "\n",
    "print ('\\n[Task 1/2] done.')\n",
    "\n",
    "rmsd_list_aligned = []\n",
    "for ts in u.trajectory[:head_frames]:\n",
    "    rmsd_ret = rms.rmsd(u.select_atoms(align_selector).positions, ref.select_atoms(align_selector).positions, superposition=False)\n",
    "    rmsd_list_aligned.append(rmsd_ret)\n",
    "\"\"\"\n",
    "\n",
    "print ('\\n[Task 2/2] done.')\n",
    "\n",
    "#print RMSD values before and after alignment\n",
    "print ('\\nfirst {} distances before alignment:\\n\\t'.format(head_frames), dist_list)\n",
    "print ('\\nfirst {} distances after alignment:\\n\\t'.format(head_frames), dist_list_aligned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7e1684",
   "metadata": {},
   "source": [
    "#### (optional) display information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8224bb33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#generate Ramachandran plot of two dihedral angles\n",
    "ax = plt.axes()\n",
    "r = dihedrals.Ramachandran(u.select_atoms('resid 2')).run()\n",
    "r.plot(ax, color='black', marker='.') #, ref=True)\n",
    "\n",
    "# display the trajectory\n",
    "view = nv.show_mdanalysis(u)\n",
    "view   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12091344",
   "metadata": {},
   "source": [
    "## Part 2: Training\n",
    "\n",
    "#### define neural network model and training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654153b8",
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#Auto encoders class and functions for training.\n",
    "def create_seqential_nn(layer_dims, activation=torch.nn.Tanh()):\n",
    "    layers = []\n",
    "    for i in range(len(layer_dims)-2) :\n",
    "        layers.append(torch.nn.Linear(layer_dims[i], layer_dims[i+1])) \n",
    "        layers.append(activation)\n",
    "    layers.append(torch.nn.Linear(layer_dims[-2], layer_dims[-1])) \n",
    "    \n",
    "    return torch.nn.Sequential(*layers).double()\n",
    "       \n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, align_func, encoder_dims, activation=torch.nn.Tanh(), atom_indices=None):\n",
    "        \"\"\"Initialise auto encoder\n",
    "\n",
    "        :param encoder_dims: list, List of dimensions for encoder, including input/output layers\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder_dims = encoder_dims\n",
    "        self.align_func = align_func\n",
    "        self.atom_indices = torch.tensor(atom_indices).long()\n",
    "        self.encoder = create_seqential_nn(encoder_dims, activation)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        # flatten the data\n",
    "        if self.atom_indices is None: # use all atoms\n",
    "            inp = torch.flatten(inp, start_dim=1)            \n",
    "        else: # use selected atoms\n",
    "            inp = torch.flatten(inp[:,self.atom_indices,:], start_dim=1)\n",
    "        encoded = self.encoder(inp)\n",
    "        return encoded\n",
    "    \n",
    "    def show_info(self):\n",
    "        print ('atom indices used in input layer: ', self.atom_indices.numpy())\n",
    "        self.align_func.show_info()\n",
    "        print ('Network:\\n', self.encoder)\n",
    "\n",
    "    def xi(self, x, is_aligned=False):\n",
    "        \"\"\"Collective variable defined through the encoder \n",
    "        \n",
    "        :return: xi: np.array\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            if torch.is_tensor(x) == False:\n",
    "                x = torch.from_numpy(x).double()\n",
    "            if is_aligned == False:\n",
    "                x = self.align_func(x)\n",
    "            return self.forward(x).detach().numpy()\n",
    "\n",
    "# Next, we define the training function \n",
    "def train(model, optimizer, writer, traj, weights, train_atom_indices, num_epochs=10, batch_size=32, test_ratio=0.2):\n",
    "    \"\"\"Function to train an AE model\n",
    "    \n",
    "    :param model: Neural network model built with PyTorch,\n",
    "    :param optimizer: PyTorch optimizer object\n",
    "    :param writer: SummaryWriter for log\n",
    "    :param traj: torch tensor, trajectory data, shape = (traj. length, no. of atom, 3)\n",
    "    :param weights: np.array, weights of each point of the trajectory when the dynamics is biased, ndim == 1\n",
    "    :param num_epochs: int, number of times the training goes through the whole dataset\n",
    "    :param batch_size: int, number of data points per batch for estimation of the gradient\n",
    "    :param test_size: float, between 0 and 1, giving the proportion of points used to compute test loss\n",
    "\n",
    "    :return: model, trained neural net model\n",
    "    :return: loss_list, list of lists of train losses and test losses; one per batch per epoch\n",
    "    \"\"\"\n",
    "    #--- prepare the data ---\n",
    "    # split the dataset into a training set (and its associated weights) and a test set\n",
    "    X_train, X_test, w_train, w_test = train_test_split(traj, weights, test_size=test_ratio)  \n",
    "    # intialization of the methods to sample with replacement from the data points (needed since weights are present)\n",
    "    train_sampler = torch.utils.data.WeightedRandomSampler(w_train, len(w_train))\n",
    "    test_sampler  = torch.utils.data.WeightedRandomSampler(w_test, len(w_test))\n",
    "    # method to construct data batches and iterate over them\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=X_train,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False,\n",
    "                                               sampler=train_sampler)\n",
    "    test_loader  = torch.utils.data.DataLoader(dataset=X_test,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False,\n",
    "                                               sampler=test_sampler)\n",
    "    \n",
    "    loss_func = torch.nn.MSELoss()\n",
    "    # --- start the training over the required number of epochs ---\n",
    "    loss_list = []\n",
    "    print (\"\\ntraining starts, %d epochs in total.\" % num_epochs) \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        # Train the model by going through the whole dataset\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for iteration, X in enumerate(train_loader):\n",
    "            # Clear gradients w.r.t. parameters\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass to get output\n",
    "            out = model(X)\n",
    "            # Evaluate loss\n",
    "            loss = loss_func(out, torch.flatten(X[:,train_atom_indices,:],start_dim=1))\n",
    "            # Get gradient with respect to parameters of the model\n",
    "            loss.backward()\n",
    "            # Store loss\n",
    "            train_loss.append(loss)\n",
    "            # Updating parameters\n",
    "            optimizer.step()\n",
    "        # Evaluate the test loss on the test dataset\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Evaluation of test loss\n",
    "            test_loss = []\n",
    "            for iteration, X in enumerate(test_loader):\n",
    "                out = model(X)\n",
    "                # Evaluate loss\n",
    "                loss = loss_func(out, torch.flatten(X[:,train_atom_indices,:],start_dim=1))\n",
    "                # Store loss\n",
    "                test_loss.append(loss)\n",
    "            loss_list.append([torch.tensor(train_loss), torch.tensor(test_loss)])\n",
    "            \n",
    "        writer.add_scalar('Loss/train', torch.mean(torch.tensor(train_loss)), epoch)\n",
    "        writer.add_scalar('Loss/test', torch.mean(torch.tensor(test_loss)), epoch)\n",
    "                        \n",
    "    print (\"training ends.\\n\") \n",
    "    \n",
    "    return model, loss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cac50a",
   "metadata": {},
   "source": [
    "#### set training parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ad270a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def set_seed_all(seed=-1):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "#All the parameters are set in the cell below. \n",
    "seed = None \n",
    "if seed:\n",
    "    set_seed_all(seed)\n",
    "  \n",
    "#set training parameters\n",
    "batch_size = 10000\n",
    "num_epochs = 10\n",
    "learning_rate = 0.005\n",
    "optimizer_algo = 'Adam'  # Adam by default, otherwise SGD\n",
    "#dimensions\n",
    "\n",
    "load_model_filename = 'checkpoint/AlanineDipeptide-2022-02-18-13:11:19/trained_model.pt' #None \n",
    "\n",
    "if load_model_filename is None:\n",
    "    train_atom_selector = \"type C or type O or type N\"\n",
    "    train_atom_ids = u.select_atoms(align_selector).ids \n",
    "    train_atom_indices = train_atom_ids - 1 # minus one, such that the index starts from 0\n",
    "\n",
    "    #input dimension\n",
    "    input_dim = 3 * len(train_atom_ids)\n",
    "    print ('{} Atoms used in define neural network:\\n'.format(len(train_atom_ids)), atoms_info.loc[atoms_info['id'].isin(train_atom_ids)][['id','name', 'type']])\n",
    "\n",
    "    # encoded dimension\n",
    "    k = 1\n",
    "    e_layer_dims = [input_dim, 20, 20, k]\n",
    "    d_layer_dims = [k, 20, 20, input_dim]\n",
    "    print ('\\nInput dim: {},\\tencoded dim: {}\\n'.format(input_dim, k))\n",
    "\n",
    "    activation = torch.nn.Tanh()\n",
    "    encoder = Encoder(align_functor, e_layer_dims, activation, train_atom_indices)\n",
    "    decoder = create_seqential_nn(d_layer_dims, activation)\n",
    "\n",
    "    ae_model = torch.nn.Sequential(encoder, decoder) \n",
    "else:\n",
    "    ae_model = torch.load(load_model_filename)\n",
    "    print (f'model loaded from: {load_model_filename}')\n",
    "    ae_model[0].show_info()\n",
    "    \n",
    "print ('Autoencoder:\\n', ae_model)\n",
    "\n",
    "# path to store log data\n",
    "model_save_dir = 'checkpoint'\n",
    "prefix = f\"{sys_name}-\" \n",
    "model_path = os.path.join(model_save_dir, prefix + time.strftime(\"%Y-%m-%d-%H:%M:%S\", time.localtime()))\n",
    "print ('\\nLog directory: {}\\n'.format(model_path))\n",
    "writer = SummaryWriter(model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80852277",
   "metadata": {},
   "source": [
    "#### start training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3751a7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "if optimizer_algo == 'Adam':\n",
    "    optimizer = torch.optim.Adam(ae_model.parameters(), lr=learning_rate)\n",
    "else:\n",
    "    optimizer = torch.optim.SGD(ae_model.parameters(), lr=learning_rate)\n",
    "\n",
    "ae_model, loss_list = train(ae_model, \n",
    "                            optimizer, \n",
    "                            writer,\n",
    "                            trajectory, \n",
    "                            np.ones(trajectory.shape[0]), \n",
    "                            train_atom_indices,\n",
    "                            batch_size=batch_size, \n",
    "                            num_epochs=num_epochs\n",
    "                            )\n",
    "\n",
    "#save the model\n",
    "trained_model_filename = f'{model_path}/trained_model.pt'\n",
    "torch.save(ae_model, trained_model_filename)  \n",
    "print (f'trained model is saved at: {trained_model_filename}\\n')\n",
    "\n",
    "#--- Compute average train per epoch ---\n",
    "loss_evol1 = []\n",
    "for i in range(len(loss_list)):\n",
    "    loss_evol1.append([torch.mean(loss_list[i][0]), torch.mean(loss_list[i][1])])\n",
    "loss_evol1 = np.array(loss_evol1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6654229",
   "metadata": {},
   "source": [
    "Plot the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24567d02",
   "metadata": {
    "lines_to_next_cell": 0,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_fig_to_file = False\n",
    "start_epoch_index = 1\n",
    "ax  = plt.axes() \n",
    "ax.plot(range(start_epoch_index, num_epochs), loss_evol1[start_epoch_index:, 0], '--', label='train loss', marker='o')\n",
    "ax.plot(range(1, num_epochs), loss_evol1[start_epoch_index:, 1], '-.', label='test loss', marker='+')\n",
    "ax.legend()\n",
    "ax.set_title('losses')\n",
    "\n",
    "if save_fig_to_file :\n",
    "    fig_filename = 'training_loss_%s.jpg' % pot_name\n",
    "    fig.savefig(fig_filename)\n",
    "    print ('training loss plotted to file: %s' % fig_filename)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python",
   "formats": "ipynb,auto:light",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
