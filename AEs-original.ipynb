{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93651423",
   "metadata": {},
   "source": [
    "### Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384fafc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import  HTML\n",
    "# Jupyter display settings\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae60866",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c7e201",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- for scientific computing ---\n",
    "import numpy as np\n",
    "from scipy import integrate\n",
    "#--- for plots ---\n",
    "import matplotlib.pyplot as plt\n",
    "\"\"\"-----------------> CHANGE AS %matplotlib inline IF YOU ARE WORKING ON GOOGLE COLAB\"\"\"\n",
    "%matplotlib notebook \n",
    "#--- for neural networks ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split as ttsplit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f0d95f",
   "metadata": {},
   "source": [
    "### Define all our potentials classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4a0011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(a):\n",
    "    \"\"\"Gaussian function\n",
    "\n",
    "    :param a: float, real value\n",
    "    :return: float, g(a)\n",
    "    \"\"\"\n",
    "    return np.exp(- a ** 2)\n",
    "\n",
    "class TripleWellPotential:\n",
    "    \"\"\"Class to gather methods related to the potential function\"\"\"\n",
    "    def __init__(self, beta):\n",
    "        \"\"\"Initialise potential function class\n",
    "\n",
    "        :param beta: float,  inverse temperature = 1 / (k_B * T)\n",
    "        :param Z: float, partition function (computed below)\n",
    "        \"\"\"\n",
    "        self.beta = beta\n",
    "        self.dim = 2\n",
    "        self.Z = None\n",
    "        \n",
    "    def V(self, X):\n",
    "        \"\"\"Potential fuction\n",
    "\n",
    "        :param X: np.array, Position  vector (x,y), ndim = 1, shape = (2,)\n",
    "        :return: V: float, potential energy value\n",
    "        \"\"\"\n",
    "        assert(type(X) == np.ndarray)\n",
    "        assert(X.ndim == 1)\n",
    "        assert(X.shape[0] == 2)\n",
    "        x = X[0]\n",
    "        y = X[1]\n",
    "        u = g(x) * (g(y - 1/3) - g(y - 5/3))\n",
    "        v = g(y) * (g(x - 1) + g(x + 1))\n",
    "        V = 3 * u - 5 * v + 0.2 * (x ** 4) + 0.2 * ((y - 1/3) ** 4)\n",
    "        return V\n",
    "    \n",
    "    def dV_x(self, x, y):\n",
    "        \"\"\"\n",
    "        :param x: float, x coordinate\n",
    "        :param y: float, y coordinate\n",
    "\n",
    "        :return: dVx: float, derivative of the potential with respect to x\n",
    "        \"\"\"\n",
    "        u = g(x) * (g(y - 1/3) - g(y - 5/3))\n",
    "        a = g(y) * ((x - 1)*g(x - 1) + (x + 1) * g(x + 1))\n",
    "        dVx = -6 * x * u + 10 * a + 0.8 * (x ** 3)\n",
    "        return dVx\n",
    "    \n",
    "    def dV_y(self, x, y):\n",
    "        \"\"\"\n",
    "        :param x: float, x coordinate\n",
    "        :param y: float, y coordinate\n",
    "\n",
    "        :return: dVy: float, derivative of the potential with respect to y\n",
    "        \"\"\"\n",
    "        u = g(x) * ((y - 1/3) * g(y - 1/3) - (y - 5/3) * g(y - 5/3))\n",
    "        b = g(y) * (g(x - 1) + g(x + 1))\n",
    "        dVy = -6 * u + 10 * y * b + 0.8 * ((y - 1/3) ** 3)\n",
    "        return dVy\n",
    "    \n",
    "    def nabla_V(self, X):\n",
    "        \"\"\"Gradient of the potential energy fuction\n",
    "\n",
    "        :param X: np.array, Position  vector (x,y), ndim = 1, shape = (2,)\n",
    "        :return: grad(X): np.array, gradient with respect to position vector (x,y), ndim = 1, shape = (2,)\n",
    "        \"\"\"\n",
    "        assert(type(X) == np.ndarray)\n",
    "        assert(X.ndim == 1)\n",
    "        assert(X.shape[0] == 2)\n",
    "        return np.array([self.dV_x(X[0], X[1]), self.dV_y(X[0], X[1])])\n",
    "        \n",
    "    def boltz_weight(self, x, y):\n",
    "        \"\"\"Compute the unnormalized weight of a configuration according to the Boltzmann distribution\n",
    "\n",
    "        :param x: float, x coordinate\n",
    "        :param y: float, y coordinate\n",
    "\n",
    "        :return: normalized Blotzmann weight\n",
    "        \"\"\"\n",
    "        X = np.array([x, y])\n",
    "        return np.exp(-self.beta * self.V(X))\n",
    "    \n",
    "    def set_Z(self):\n",
    "        \"\"\"Partition function to normalize probability densities\n",
    "        \"\"\"\n",
    "        self.Z, _ = integrate.dblquad(self.boltz_weight, -5, 5, -5, 5)\n",
    "        \n",
    "        \n",
    "class TripleWellOneChannelPotential:\n",
    "    \"\"\"Class to gather methods related to the potential function\"\"\"\n",
    "    def __init__(self, beta):\n",
    "        \"\"\"Initialise potential function class\n",
    "\n",
    "        :param beta: float,  inverse temperature = 1 / (k_B * T)\n",
    "        :param Z: float, partition function (computed below)\n",
    "        \"\"\"\n",
    "        self.beta = beta\n",
    "        self.dim = 2\n",
    "        self.Z = None\n",
    "        \n",
    "    def V(self, X):\n",
    "        \"\"\"Potential fuction\n",
    "\n",
    "        :param X: np.array, Position  vector (x,y), ndim = 1, shape = (2,)\n",
    "        :return: V: float, potential energy value\n",
    "        \"\"\"\n",
    "        assert(type(X) == np.ndarray)\n",
    "        assert(X.ndim == 1)\n",
    "        assert(X.shape[0] == 2)   \n",
    "        a = + 10 * np.exp(- 25 * X[0] ** 2 - (X[1] + (1 / 3)) ** 2)\n",
    "        b = - 3 * np.exp(- X[0] ** 2 - (X[1] - (5 / 3)) ** 2)\n",
    "        c = - 5 * np.exp(- X[1] ** 2 - (X[0] - 1) ** 2)\n",
    "        d = - 5 * np.exp(- X[1] ** 2 - (X[0] + 1) ** 2) \n",
    "        e = + 0.2 * X[0] ** 4 + 0.2 * (X[1] - (1 / 3)) ** 4\n",
    "        V = a + b + c + d + e \n",
    "        return V\n",
    "    \n",
    "    def dV_x(self, x, y):\n",
    "        \"\"\"\n",
    "        :param x: float, x coordinate\n",
    "        :param y: float, y coordinate\n",
    "\n",
    "        :return: dVx: float, derivative of the potential with respect to x\n",
    "        \"\"\"\n",
    "        a = - 20 * 25 * x * np.exp(- 25 * x ** 2 - (y + (1 / 3)) ** 2)\n",
    "        b = + 6 * x * np.exp(- x ** 2 - (y - (5 / 3)) ** 2)\n",
    "        c = + 10 * (x - 1) * np.exp(- y ** 2 - (x - 1) ** 2)\n",
    "        d = + 10 * (x + 1) * np.exp(- y ** 2 - (x + 1) ** 2) \n",
    "        e = + 0.8 * x ** 3\n",
    "        dVx = a + b + c + d + e\n",
    "        return dVx\n",
    "    \n",
    "    def dV_y(self, x, y):\n",
    "        \"\"\"\n",
    "        :param x: float, x coordinate\n",
    "        :param y: float, y coordinate\n",
    "\n",
    "        :return: dVy: float, derivative of the potential with respect to y\n",
    "        \"\"\"\n",
    "        a = - 20 * (y + (1 / 3)) * np.exp(- 25 * x ** 2 - (y + (1 / 3)) ** 2)\n",
    "        b = + 6 * (y - (5 / 3)) * np.exp(- x ** 2 - (y - (5 / 3)) ** 2)\n",
    "        c = + 10 * y * np.exp(- y ** 2 - (x - 1) ** 2)\n",
    "        d = + 10 * y * np.exp(- y ** 2 - (x + 1) ** 2) \n",
    "        e = + 0.8 * y ** 3\n",
    "        dVy = a + b + c + d + e\n",
    "        return dVy\n",
    "    \n",
    "    def nabla_V(self, X):\n",
    "        \"\"\"Gradient of the potential energy fuction\n",
    "\n",
    "        :param X: np.array, Position  vector (x,y), ndim = 1, shape = (2,)\n",
    "        :return: grad(X): np.array, gradient with respect to position vector (x,y), ndim = 1, shape = (2,)\n",
    "        \"\"\"\n",
    "        assert(type(X) == np.ndarray)\n",
    "        assert(X.ndim == 1)\n",
    "        assert(X.shape[0] == 2)\n",
    "        return np.array([self.dV_x(X[0], X[1]), self.dV_y(X[0], X[1])])\n",
    "        \n",
    "    def boltz_weight(self, x, y):\n",
    "        \"\"\"Compute the unnormalized weight of a configuration according to the Boltzmann distribution\n",
    "\n",
    "        :param x: float, x coordinate\n",
    "        :param y: float, y coordinate\n",
    "\n",
    "        :return: normalized Blotzmann weight\n",
    "        \"\"\"\n",
    "        X = np.array([x, y])\n",
    "        return np.exp(-self.beta * self.V(X))\n",
    "    \n",
    "    def set_Z(self):\n",
    "        \"\"\"Partition function to normalize probability densities\n",
    "        \"\"\"\n",
    "        self.Z, _ = integrate.dblquad(self.boltz_weight, -5, 5, -5, 5)\n",
    "        \n",
    "class DoubleWellPotential:\n",
    "    \"\"\"Class to gather methods related to the potential function\"\"\"\n",
    "    def __init__(self, beta):\n",
    "        \"\"\"Initialise potential function class\n",
    "\n",
    "        :param beta: float,  inverse temperature = 1 / (k_B * T)\n",
    "        :param Z: float, partition function (computed below)\n",
    "        \"\"\"\n",
    "        self.beta = beta\n",
    "        self.dim = 2\n",
    "        self.Z = None\n",
    "        \n",
    "    def V(self, X):\n",
    "        \"\"\"Potential fuction\n",
    "\n",
    "        :param X: np.array, Position  vector (x,y), ndim = 1, shape = (2,)\n",
    "        :return: V: float, potential energy value\n",
    "        \"\"\"\n",
    "        assert(type(X) == np.ndarray)\n",
    "        assert(X.ndim == 1)\n",
    "        assert(X.shape[0] == 2)\n",
    "        a = 3 * np.exp(- X[0] ** 2 - X[1] ** 2)\n",
    "        b = - 5 * np.exp(- X[1] ** 2 - (X[0] - 1) ** 2)\n",
    "        c = - 5 * np.exp(- X[1] ** 2 - (X[0] + 1) ** 2)\n",
    "        d = + 0.2 * X[0] ** 4 + 0.2 * X[1] ** 4\n",
    "        V = a + b + c + d\n",
    "        return V\n",
    "    \n",
    "    def dV_x(self, x, y):\n",
    "        \"\"\"\n",
    "        :param x: float, x coordinate\n",
    "        :param y: float, y coordinate\n",
    "\n",
    "        :return: dVx: float, derivative of the potential with respect to x\n",
    "        \"\"\"   \n",
    "        a = -6 * x * np.exp(- x ** 2 - y ** 2)\n",
    "        b = + 10 * (x - 1) * np.exp(- y ** 2 - (x - 1) ** 2)\n",
    "        d = + 10 * (x + 1) * np.exp(- y ** 2 - (x + 1) ** 2)\n",
    "        c = + 0.8 * x ** 3\n",
    "        dVx = a + b + c + d\n",
    "        return dVx\n",
    "    \n",
    "    def dV_y(self, x, y):\n",
    "        \"\"\"\n",
    "        :param x: float, x coordinate\n",
    "        :param y: float, y coordinate\n",
    "\n",
    "        :return: dVy: float, derivative of the potential with respect to y\n",
    "        \"\"\"\n",
    "        a = - 6 * y * np.exp(- x ** 2 - y ** 2) \n",
    "        b = + 10 * y * np.exp(- y ** 2 - (x - 1) ** 2)\n",
    "        c = + 10 * y * np.exp(- y ** 2 - (x + 1) ** 2)\n",
    "        d = + 0.8 * y ** 3\n",
    "        dVy = a + b + c + d\n",
    "        return dVy\n",
    "    \n",
    "    def nabla_V(self, X):\n",
    "        \"\"\"Gradient of the potential energy fuction\n",
    "\n",
    "        :param X: np.array, Position  vector (x,y), ndim = 1, shape = (2,)\n",
    "        :return: grad(X): np.array, gradient with respect to position vector (x,y), ndim = 1, shape = (2,)\n",
    "        \"\"\"\n",
    "        assert(type(X) == np.ndarray)\n",
    "        assert(X.ndim == 1)\n",
    "        assert(X.shape[0] == 2)\n",
    "        return np.array([self.dV_x(X[0], X[1]), self.dV_y(X[0], X[1])])\n",
    "        \n",
    "    def boltz_weight(self, x, y):\n",
    "        \"\"\"Compute the unnormalized weight of a configuration according to the Boltzmann distribution\n",
    "\n",
    "        :param x: float, x coordinate\n",
    "        :param y: float, y coordinate\n",
    "\n",
    "        :return: normalized Blotzmann weight\n",
    "        \"\"\"\n",
    "        X = np.array([x, y])\n",
    "        return np.exp(-self.beta * self.V(X))\n",
    "    \n",
    "    def set_Z(self):\n",
    "        \"\"\"Partition function to normalize probability densities\n",
    "        \"\"\"\n",
    "        self.Z, _ = integrate.dblquad(self.boltz_weight, -5, 5, -5, 5)\n",
    "        \n",
    "class ZPotential:\n",
    "    \"\"\"Class to gather methods related to the potential function\"\"\"\n",
    "    def __init__(self, beta):\n",
    "        \"\"\"Initialise potential function class\n",
    "\n",
    "        :param beta: float,  inverse temperature = 1 / (k_B * T)\n",
    "        :param Z: float, partition function (computed below)\n",
    "        \"\"\"\n",
    "        self.beta = beta\n",
    "        self.dim = 2\n",
    "        self.Z = None\n",
    "        \n",
    "    def V(self, X):\n",
    "        \"\"\"Potential fuction\n",
    "\n",
    "        :param X: np.array, Position  vector (x,y), ndim = 1, shape = (2,)\n",
    "        :return: V: float, potential energy value\n",
    "        \"\"\"\n",
    "        assert(type(X) == np.ndarray)\n",
    "        assert(X.ndim == 1)\n",
    "        assert(X.shape[0] == 2)\n",
    "        a = - 3 * np.exp(- 0.01 * (X[0] + 5) ** 2 - 0.2 * (X[1] + 5) ** 2) \n",
    "        b = - 3 * np.exp(- 0.01 * (X[0] - 5) ** 2 - 0.2 * (X[1] - 5) ** 2)\n",
    "        c = + 5 * np.exp(- 0.20 * (X[0] + 3 * (X[1] - 3)) ** 2) / (1 + np.exp(- X[0] - 3))\n",
    "        d = + 5 * np.exp(- 0.20 * (X[0] + 3 * (X[1] + 3)) ** 2) / (1 + np.exp(+ X[0] - 3))\n",
    "        e = + 3 * np.exp(- 0.01 * (X[0] ** 2 + X[1] ** 2))\n",
    "        f = (X[0] ** 4 + X[1] ** 4) / 20480\n",
    "        V = a + b + c + d + e + f\n",
    "        return V\n",
    "    \n",
    "    def dV_x(self, x, y):\n",
    "        \"\"\"\n",
    "        :param x: float, x coordinate\n",
    "        :param y: float, y coordinate\n",
    "\n",
    "        :return: dVx: float, derivative of the potential with respect to x\n",
    "        \"\"\"    \n",
    "        a = + 0.06 * (x + 5) * np.exp(- 0.01 * (x + 5) ** 2 - 0.2 * (y + 5) ** 2)\n",
    "        b = + 0.06 * (x - 5) * np.exp(- 0.01 * (x - 5) ** 2 - 0.2 * (y - 5) ** 2)\n",
    "        d = + (5 / (1 + np.exp(- x - 3)) ** 2) * (+ np.exp(- x - 3) * np.exp(- 0.2 * (x + 3 * (y - 3)) ** 2) - 0.4 * (x + 3 * (y - 3)) * np.exp(-0.2 * (x + 3 * (y - 3)) ** 2) * (1 + np.exp(- x - 3)))\n",
    "        c = + (5 / (1 + np.exp(+ x - 3)) ** 2) * (- np.exp(+ x - 3) * np.exp(- 0.2 * (x + 3 * (y + 3)) ** 2) - 0.4 * (x + 3 * (y + 3)) * np.exp(-0.2 * (x + 3 * (y + 3)) ** 2) * (1 + np.exp(+ x - 3)))\n",
    "        e = - 0.06 * x * np.exp(- 0.01 * (x ** 2 + y ** 2))\n",
    "        f = (4 * x ** 3) / 20480\n",
    "        dVx = a + b + c + d + e + f  \n",
    "        return dVx\n",
    "    \n",
    "    def dV_y(self, x, y):\n",
    "        \"\"\"\n",
    "        :param x: float, x coordinate\n",
    "        :param y: float, y coordinate\n",
    "\n",
    "        :return: dVy: float, derivative of the potential with respect to y\n",
    "        \"\"\"   \n",
    "        a = + 1.2 * (y + 5) * np.exp(- 0.01 * (x + 5) ** 2 - 0.2 * (y + 5) ** 2)\n",
    "        b = + 1.2 * (y - 5) * np.exp(- 0.01 * (x - 5) ** 2 - 0.2 * (y - 5) ** 2)\n",
    "        c = - (5 / (1 + np.exp(- x - 3))) * 1.2 * (x + 3 * (y - 3)) * np.exp(- 0.2 * (x + 3 * (y - 3)) ** 2)\n",
    "        d = - (5 / (1 + np.exp(+ x - 3))) * 1.2 * (x + 3 * (y + 3)) * np.exp(- 0.2 * (x + 3 * (y + 3)) ** 2)\n",
    "        e = -  0.06 * y * np.exp(- 0.01 *(x ** 2 + y ** 2))\n",
    "        f = (4 * y ** 3) / 20480\n",
    "        dVy = a + b + c + d + e + f\n",
    "        return dVy\n",
    "    \n",
    "    def nabla_V(self, X):\n",
    "        \"\"\"Gradient of the potential energy fuction\n",
    "\n",
    "        :param X: np.array, Position  vector (x,y), ndim = 1, shape = (2,)\n",
    "        :return: grad(X): np.array, gradient with respect to position vector (x,y), ndim = 1, shape = (2,)\n",
    "        \"\"\"\n",
    "        assert(type(X) == np.ndarray)\n",
    "        assert(X.ndim == 1)\n",
    "        assert(X.shape[0] == 2)\n",
    "        return np.array([self.dV_x(X[0], X[1]), self.dV_y(X[0], X[1])])\n",
    "        \n",
    "    def boltz_weight(self, x, y):\n",
    "        \"\"\"Compute the unnormalized weight of a configuration according to the Boltzmann distribution\n",
    "\n",
    "        :param x: float, x coordinate\n",
    "        :param y: float, y coordinate\n",
    "\n",
    "        :return: normalized Blotzmann weight\n",
    "        \"\"\"\n",
    "        X = np.array([x, y])\n",
    "        return np.exp(-self.beta * self.V(X))\n",
    "    \n",
    "    def set_Z(self):\n",
    "        \"\"\"Partition function to normalize probability densities\n",
    "        \"\"\"\n",
    "        self.Z, _ = integrate.dblquad(self.boltz_weight, -20, 20, -20, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc412e9",
   "metadata": {},
   "source": [
    "We visualise our different potentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5284197d",
   "metadata": {},
   "outputs": [],
   "source": [
    "twpot = TripleWellPotential(1)\n",
    "grid = np.linspace(-2,2,100)\n",
    "x_plot_tw = np.outer(grid, np.ones(100))\n",
    "y_plot_tw = np.outer(grid + 0.5, np.ones(100)).T\n",
    "twpotential_on_grid = np.zeros([100, 100])\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        twpotential_on_grid[i, j] = twpot.V(np.array([grid[i], grid[j] + 0.5]))\n",
    "\n",
    "fig0 = plt.figure(figsize=(9,3))\n",
    "ax0 = fig0.add_subplot(1, 2, 1, projection='3d')\n",
    "ax1 = fig0.add_subplot(1, 2, 2)\n",
    "ax0.plot_surface(x_plot_tw, y_plot_tw, twpotential_on_grid,cmap='coolwarm_r', edgecolor='none')\n",
    "ax0.set_title('Tripple Well Potential')\n",
    "ax1.pcolormesh(x_plot_tw, y_plot_tw, twpotential_on_grid, cmap='coolwarm_r',shading='auto')\n",
    "\n",
    "twocpot = TripleWellOneChannelPotential(1)\n",
    "grid = np.linspace(-2,2,100)\n",
    "x_plot_tw = np.outer(grid, np.ones(100))\n",
    "y_plot_tw = np.outer(grid + 0.5, np.ones(100)).T\n",
    "twocpotential_on_grid = np.zeros([100, 100])\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        twocpotential_on_grid[i, j] = twocpot.V(np.array([grid[i], grid[j] + 0.5]))\n",
    "\n",
    "fig1 = plt.figure(figsize=(9,3))\n",
    "ax0 = fig1.add_subplot(1, 2, 1, projection='3d')\n",
    "ax1 = fig1.add_subplot(1, 2, 2)\n",
    "ax0.set_title('Tripple Well Oce Channel Potential')\n",
    "ax0.plot_surface(x_plot_tw, y_plot_tw, twocpotential_on_grid,cmap='coolwarm_r', edgecolor='none')\n",
    "ax1.pcolormesh(x_plot_tw, y_plot_tw, twocpotential_on_grid, cmap='coolwarm_r',shading='auto')\n",
    "\n",
    "dwpot = DoubleWellPotential(1)\n",
    "grid = np.linspace(-2,2,100)\n",
    "x_plot_dw = np.outer(grid, np.ones(100))\n",
    "y_plot_dw = np.outer(grid, np.ones(100)).T\n",
    "dwpotential_on_grid = np.zeros([100, 100])\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        dwpotential_on_grid[i, j] = dwpot.V(np.array([grid[i], grid[j]]))\n",
    "        \n",
    "fig2 = plt.figure(figsize=(9,3))\n",
    "ax0 = fig2.add_subplot(1, 2, 1, projection='3d')\n",
    "ax1 = fig2.add_subplot(1, 2, 2)\n",
    "ax0.set_title('Doubble Well Potential')\n",
    "ax0.plot_surface(x_plot_dw, y_plot_dw, dwpotential_on_grid,cmap='coolwarm_r', edgecolor='none')\n",
    "ax1.pcolormesh(x_plot_dw, y_plot_dw, dwpotential_on_grid, cmap='coolwarm_r',shading='auto')\n",
    "\n",
    "zwpot = ZPotential(1)\n",
    "grid = np.linspace(-15,15,100)\n",
    "x_plot_z = np.outer(grid, np.ones(100))\n",
    "y_plot_z = np.outer(grid, np.ones(100)).T\n",
    "zpotential_on_grid = np.zeros([100, 100])\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        zpotential_on_grid[i, j] = zwpot.V(np.array([grid[i], grid[j]]))\n",
    "        \n",
    "fig3 = plt.figure(figsize=(9,3))\n",
    "ax0 = fig3.add_subplot(1, 2, 1, projection='3d')\n",
    "ax1 = fig3.add_subplot(1, 2, 2)\n",
    "ax0.set_title('Z Potential')\n",
    "ax0.plot_surface(x_plot_z, y_plot_z, zpotential_on_grid,cmap='coolwarm_r', edgecolor='none')\n",
    "ax1.pcolormesh(x_plot_z, y_plot_z, zpotential_on_grid, cmap='coolwarm_r',shading='auto')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163015d1",
   "metadata": {},
   "source": [
    "We first define a function 'UnbiasedTraj' to generate an trajectory according an Euler--Maruyama discretization \n",
    "$$\n",
    "X^{n+1} = X^n - \\Delta t \\nabla V(X^n) + \\sqrt{\\frac{2 \\Delta t}{\\beta}} \\, G^n \n",
    "$$\n",
    "of the overdamped Langevin dynamics\n",
    "$$\n",
    "dX_t = -\\nabla V(X_t) \\, dt + \\sqrt{\\frac{2}{\\beta}} \\, dW_t\n",
    "$$\n",
    "This functions takes as argument a potential object, initial conditions, the number of simulation steps and a time step. It generates a realization of a trajectory (subsampled at some prescribed rate), and possibly records the value of the potential energy function at the points along the trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13bb91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def UnbiasedTraj(pot, X_0, delta_t=1e-3, T=1000, save=1, save_energy=False, seed=0):\n",
    "    \"\"\"Simulates an overdamped langevin trajectory with a Euler-Maruyama numerical scheme \n",
    "\n",
    "    :param pot: potential object, must have methods for energy gradient and energy evaluation\n",
    "    :param X_0: Initial position, must be a 2D vector\n",
    "    :param delta_t: Discretization time step\n",
    "    :param T: Number of points in the trajectory (the total simulation time is therefore T * delta_t)\n",
    "    :param save: Integer giving the period (counted in number of steps) at which the trajectory is saved\n",
    "    :param save_energy: Boolean parameter to save energy along the trajectory\n",
    "\n",
    "    :return: traj: np.array with ndim = 2 and shape = (T // save + 1, 2)\n",
    "    :return: Pot_values: np.array with ndim = 2 and shape = (T // save + 1, 1)\n",
    "    \"\"\"\n",
    "    r = np.random.RandomState(seed)\n",
    "    X = X_0\n",
    "    dim = X.shape[0]\n",
    "    traj = [X]\n",
    "    if save_energy:\n",
    "        Pot_values = [pot.V(X)]\n",
    "    else:\n",
    "        Pot_values = None\n",
    "    for i in range(T):\n",
    "        b = r.normal(size=(dim,))\n",
    "        X = X - pot.nabla_V(X) * delta_t + np.sqrt(2 * delta_t/pot.beta) * b\n",
    "        if i % save==0:\n",
    "            traj.append(X)\n",
    "            if save_energy:\n",
    "                Pot_values.append(pot.V(X))\n",
    "    return np.array(traj), np.array(Pot_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d4e405",
   "metadata": {},
   "source": [
    "We now define the Auto encoders classes and usefull functions for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e85019",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim,bottleneck_dim):\n",
    "        \"\"\"Initialise simplest autoencoder (input->bottleneck->ouput), with hyperbolic tangent activation function\n",
    "       \n",
    "        :param input_dim: int, Number of dimension of the input vectors\n",
    "        :param bottleneck_dim: int, Number of dimension of the bottleneck\n",
    "        \"\"\"\n",
    "        super(SimpleAutoEncoder, self).__init__()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, bottleneck_dim),\n",
    "            torch.nn.Tanh()\n",
    "        )\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(bottleneck_dim, input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, inp):\n",
    "        encoded = self.encoder(inp)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "class DeepAutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, bottleneck_dim):\n",
    "        \"\"\"Initialise auto encoder with hyperbolic tangent activation function\n",
    "        You can uncomment certain lines in the encoder and decoder functions to modify the topology of the network\n",
    "        Make sure when you initialise the AE object that the list 'hidden_dims' has a length consistent with the architecture\n",
    "\n",
    "        :param input_dim: int, Number of dimension of the input vectors\n",
    "        :param hidden_dims: list, List of hidden layers\n",
    "        :param bottleneck_dim: int, Number of dimension of the bottleneck\n",
    "        \"\"\"\n",
    "        super(DeepAutoEncoder, self).__init__()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, hidden_dims[0]),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(hidden_dims[0], hidden_dims[1]),\n",
    "            torch.nn.Tanh(),\n",
    "            # torch.nn.Linear(hidden_dims[1], hidden_dims[2]),\n",
    "            # torch.nn.Tanh(),\n",
    "            # torch.nn.Linear(hidden_dims[2], hidden_dims[3]),\n",
    "            # torch.nn.Tanh(),\n",
    "            # torch.nn.Linear(hidden_dims[3], hidden_dims[4]),\n",
    "            # torch.nn.Tanh(),\n",
    "            torch.nn.Linear(hidden_dims[-1], bottleneck_dim),\n",
    "            torch.nn.Tanh()\n",
    "        )\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(bottleneck_dim, hidden_dims[-1]),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(hidden_dims[-1], hidden_dims[-2]),\n",
    "            torch.nn.Tanh(),\n",
    "            # torch.nn.Linear(hidden_dims[-2], hidden_dims[-3]),\n",
    "            # torch.nn.Tanh(),\n",
    "            # torch.nn.Linear(hidden_dims[-3], hidden_dims[-4]),\n",
    "            # torch.nn.Tanh(),\n",
    "            # torch.nn.Linear(hidden_dims[-4], hidden_dims[-5]),\n",
    "            # torch.nn.Tanh(),\n",
    "            torch.nn.Linear(hidden_dims[0], input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, inp):\n",
    "        # Input Linear function\n",
    "        encoded = self.encoder(inp)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "class AssymmetricAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AssymmetricAutoEncoder, self).__init__()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2, 1),\n",
    "            torch.nn.Tanh()\n",
    "        )\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(1, 2),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(2, 3),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(3, 4),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(4, 3),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(3, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, inp):\n",
    "        encoded = self.encoder(inp)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "    \n",
    "def set_learning_parameters(model, learning_rate, loss='MSE', optimizer='Adam'):\n",
    "    \"\"\"Function to set learning parameter\n",
    "\n",
    "    :param model: Neural network model build with PyTorch,\n",
    "    :param learning_rate: Value of the learning rate\n",
    "    :param loss: String, type of loss desired ('MSE' by default, another choice leads to L1 loss)\n",
    "    :param optimizer: String, type of optimizer ('Adam' by default, another choice leads to SGD)\n",
    "\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    #--- chosen loss function ---\n",
    "    if loss == 'MSE':\n",
    "        loss_function = nn.MSELoss()\n",
    "    else:\n",
    "        loss_function = nn.L1Loss()\n",
    "    #--- chosen optimizer ---\n",
    "    if optimizer == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    return loss_function, optimizer\n",
    "\n",
    "def train_AE(model, loss_function, optimizer, traj, weights, num_epochs=10, batch_size=32, test_size=0.2):\n",
    "    \"\"\"Function to train an AE model\n",
    "\n",
    "    :param model: Neural network model built with PyTorch,\n",
    "    :param loss_function: Function built with PyTorch tensors or built-in PyTorch loss function\n",
    "    :param optimizer: PyTorch optimizer object\n",
    "    :param traj: np.array, physical trajectory (in the potential pot), ndim == 2, shape == T // save + 1, pot.dim\n",
    "    :param weights: np.array, weights of each point of the trajectory when the dynamics is biased, ndim == 1, shape == T // save + 1, 1\n",
    "    :param num_epochs: int, number of times the training goes through the whole dataset\n",
    "    :param batch_size: int, number of data points per batch for estimation of the gradient\n",
    "    :param test_size: float, between 0 and 1, giving the proportion of points used to compute test loss\n",
    "\n",
    "    :return: model, trained neural net model\n",
    "    :return: training_data, list of lists of train losses and test losses; one per batch per epoch\n",
    "    \"\"\"\n",
    "    #--- prepare the data ---\n",
    "    # split the dataset into a training set (and its associated weights) and a test set\n",
    "    X_train, X_test, w_train, w_test = ttsplit(traj, weights, test_size=test_size)\n",
    "    X_train = torch.tensor(X_train.astype('float32'))\n",
    "    X_test = torch.tensor(X_test.astype('float32'))\n",
    "    w_train = torch.tensor(w_train.astype('float32'))\n",
    "    w_test = torch.tensor(w_test.astype('float32'))\n",
    "    # intialization of the methods to sample with replacement from the data points (needed since weights are present)\n",
    "    train_sampler = torch.utils.data.WeightedRandomSampler(w_train, len(w_train))\n",
    "    test_sampler  = torch.utils.data.WeightedRandomSampler(w_test, len(w_test))\n",
    "    # method to construct data batches and iterate over them\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=X_train,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False,\n",
    "                                               sampler=train_sampler)\n",
    "    test_loader  = torch.utils.data.DataLoader(dataset=X_test,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False,\n",
    "                                               sampler=test_sampler)\n",
    "    \n",
    "    #--- start the training over the required number of epochs ---\n",
    "    training_data = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train the model by going through the whole dataset\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for iteration, X in enumerate(train_loader):\n",
    "            # Set gradient calculation capabilities\n",
    "            X.requires_grad_()\n",
    "            # Clear gradients w.r.t. parameters\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass to get output\n",
    "            out = model(X)\n",
    "            # Evaluate loss\n",
    "            loss = loss_function(out, X)\n",
    "            # Store loss\n",
    "            train_loss.append(loss)\n",
    "            # Get gradient with respect to parameters of the model\n",
    "            loss.backward()\n",
    "            # Updating parameters\n",
    "            optimizer.step()\n",
    "        # Evaluate the test loss on the test dataset\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_loss = []\n",
    "            for iteration, X in enumerate(test_loader):\n",
    "                out = model(X)\n",
    "                # Evaluate loss\n",
    "                loss = loss_function(out, X)\n",
    "                # Store loss\n",
    "                test_loss.append(loss)\n",
    "            training_data.append([torch.tensor(train_loss), torch.tensor(test_loss)])\n",
    "    return model, training_data\n",
    "\n",
    "def xi_ae(model,  x):\n",
    "    \"\"\"Collective variable defined through an auto encoder model\n",
    "\n",
    "    :param model: Neural network model build with PyTorch\n",
    "    :param x: np.array, position, ndim = 2, shape = (1,1)\n",
    "\n",
    "    :return: xi: np.array\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    x = torch.tensor(x.astype('float32'))\n",
    "    return model.encoder(x).detach().numpy()\n",
    "\n",
    "def grad_xi_ae(model, x):\n",
    "    \"\"\"Gradient of the collective variable defined through an auto encoder model\n",
    "\n",
    "    :param model: Neural network model build with pytorch,\n",
    "    :param x: np.array, position, ndim = 2, shape = (1,1)\n",
    "\n",
    "    :return: grad_xi: np.array\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    x = torch.tensor(x.astype('float32'))\n",
    "    x.requires_grad_()\n",
    "    enc = model.encoder(x)\n",
    "    grad = torch.autograd.grad(enc, x)[0]\n",
    "    return grad.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ad66e9",
   "metadata": {},
   "source": [
    "We define in the next cell an alternative training function to compute and store values for the \"variance interpretation\" of the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bec844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_variance_decomposition_plots(model,\n",
    "                                            loss_function,\n",
    "                                            optimizer,\n",
    "                                            traj,\n",
    "                                            weights,\n",
    "                                            num_epochs=10,\n",
    "                                            batch_size=32,\n",
    "                                            test_size=0.2,\n",
    "                                            n_bins_z=20,\n",
    "                                            x_domain=[-2, 2],\n",
    "                                            y_domain=[-1.5, 2.5]\n",
    "                                           ):\n",
    "    \"\"\"Function to train an AE model\n",
    "    \n",
    "    :param model: Neural network model built with PyTorch,\n",
    "    :param loss_function: Function built with PyTorch tensors or built-in PyTorch loss function\n",
    "    :param optimizer: PyTorch optimizer object\n",
    "    :param traj: np.array, physical trajectory (in the potential pot), ndim == 2, shape == T // save + 1, pot.dim\n",
    "    :param weights: np.array, weights of each point of the trajectory when the dynamics is biased, ndim == 1, shape == T // save + 1, 1\n",
    "    :param num_epochs: int, number of times the training goes through the whole dataset\n",
    "    :param batch_size: int, number of data points per batch for estimation of the gradient\n",
    "    :param test_size: float, between 0 and 1, giving the proportion of points used to compute test loss\n",
    "    :param n_bins_z: integer, number of bins in the z coordinat\n",
    "    :param x_domain: list, min and max value of x to define the interval of variation of the encoded values\n",
    "    :param y_domain: list, min and max value of y to define the interval of variation of the encoded values\n",
    "\n",
    "    :return: model, trained neural net model\n",
    "    :return: training_data, list of lists of train losses and test losses; one per batch per epoch\n",
    "    :return: X_given_z, list giving, for each epoch, a list where, for each bin in z, a list of X vectors is provided \n",
    "    :return: z_bins, list giving, for each epoch, the list of bins centers in z (correspond to a grid)\n",
    "    :return: Esp_X_given_z, list giving, for each epoch, a list of averages of X conditioned on z corresponding \n",
    "    in each z-bin\n",
    "    :return: Grad_Esp_X_given_z, list giving, for each epoch, the list of values of nabla xi evaluated \n",
    "    at the averages of X conditioned on z\n",
    "    :return: Std1_X_given_z, list giving, for each epoch, the list of standard deviations of X given z in each z-bin\n",
    "    :return: Std2_X_given_z, same as above, but with components being the std in each direction\n",
    "    :return: f_dec_z, list giving, for each epoch, a list of decoded values of z for each z-bin\n",
    "    \"\"\"\n",
    "    #--- prepare the data ---\n",
    "    # split the dataset into a training set (and its associated weights) and a test set\n",
    "    X_train, X_test, w_train, w_test = ttsplit(traj, weights, test_size=test_size)\n",
    "    X_train = torch.tensor(X_train.astype('float32'))\n",
    "    X_test = torch.tensor(X_test.astype('float32'))\n",
    "    w_train = torch.tensor(w_train.astype('float32'))\n",
    "    w_test = torch.tensor(w_test.astype('float32'))\n",
    "    # intialization of the methods to sample with replacement from the data points (needed since weights are present)\n",
    "    train_sampler = torch.utils.data.WeightedRandomSampler(w_train, len(w_train))\n",
    "    test_sampler  = torch.utils.data.WeightedRandomSampler(w_test, len(w_test))\n",
    "    # method to construct data batches and iterate over them\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=X_train,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False,\n",
    "                                               sampler=train_sampler)\n",
    "    test_loader  = torch.utils.data.DataLoader(dataset=X_test,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False,\n",
    "                                               sampler=test_sampler)\n",
    "    \n",
    "    #--- Prepare empty list to store X given z ---\n",
    "    X_given_z          = [[[] for i in range(n_bins_z)] for j in range(num_epochs)]\n",
    "    Esp_X_given_z      = [[np.zeros(2) for i in range(n_bins_z)] for j in range(num_epochs)]\n",
    "    Grad_Esp_X_given_z = [[np.zeros(2) for i in range(n_bins_z)] for j in range(num_epochs)]\n",
    "    Std1_X_given_z     = [[np.zeros(1) for i in range(n_bins_z)] for j in range(num_epochs)]\n",
    "    Std2_X_given_z     = [[np.zeros(2) for i in range(n_bins_z)] for j in range(num_epochs)]\n",
    "    z_bins = [[] for j in range(num_epochs)]\n",
    "    f_dec_z = [[np.zeros(2) for i in range(n_bins_z)] for j in range(num_epochs)]\n",
    "    # --- start the training over the required number of epochs ---\n",
    "    training_data = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train the model by going through the whole dataset\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for iteration, X in enumerate(train_loader):\n",
    "            # Set gradient calculation capabilities\n",
    "            X.requires_grad_()\n",
    "            # Clear gradients w.r.t. parameters\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass to get output\n",
    "            out = model(X)\n",
    "            # Evaluate loss\n",
    "            loss = loss_function(out, X)\n",
    "            # Get gradient with respect to parameters of the model\n",
    "            loss.backward()\n",
    "            # Store loss\n",
    "            train_loss.append(loss)\n",
    "            # Updating parameters\n",
    "            optimizer.step()\n",
    "        # Evaluate the test loss on the test dataset and preparation to compute conditional properties\n",
    "        model.eval()\n",
    "        # Initialize extremal values of z over data set to determine location of bins\n",
    "        z_min = 1\n",
    "        z_max = -1\n",
    "        with torch.no_grad():\n",
    "            # Evaluation of test loss\n",
    "            test_loss = []\n",
    "            for iteration, X in enumerate(test_loader):\n",
    "                out = model(X)\n",
    "                # Evaluate loss\n",
    "                loss = loss_function(out, X)\n",
    "                # Store loss\n",
    "                test_loss.append(loss)\n",
    "            training_data.append([torch.tensor(train_loss), torch.tensor(test_loss)])\n",
    "            # Preparation for computation of conditional properties : first bounds on z, then sorting out\n",
    "            gridx = np.linspace(x_domain[0], x_domain[1],100)\n",
    "            gridy = np.linspace(y_domain[0], y_domain[1],100)\n",
    "            xi_ae1_on_grid = np.zeros([100, 100])\n",
    "            for i in range(100):\n",
    "                for j in range(100):\n",
    "                    x = np.array([gridx[i], gridy[j]])\n",
    "                    z = xi_ae(model, x)\n",
    "                    # Update z_min and z_max\n",
    "                    if z > z_max:\n",
    "                        z_max = z\n",
    "                    if z < z_min:\n",
    "                        z_min = z\n",
    "            dz = np.array((z_max - z_min)/n_bins_z)\n",
    "            for x in X_train:\n",
    "                z = model.encoder(x)\n",
    "                h = int((z - z_min) / dz)\n",
    "                # remove one point, the one for which z=z_max (in which case h=n_bins_z, out of bounds)\n",
    "                if (h < n_bins_z):\n",
    "                    X_given_z[epoch][h].append(x)\n",
    "        #--- Computation of conditional properties ---\n",
    "        z_bins[epoch].append((np.linspace(z_min, z_max, n_bins_z)).tolist())\n",
    "        for j in range(n_bins_z):       \n",
    "            # test whether there are elements X in the j-th bin, and if yes, compute conditional properties \n",
    "            if len(X_given_z[epoch][j]) > 0:\n",
    "                # transform PyTorch tensors into numpy arrays\n",
    "                values =  np.array([np.array(X_given_z[epoch][j][0])])\n",
    "                for i in range(1, len(X_given_z[epoch][j])):\n",
    "                    values = np.append(values, np.array([np.array(X_given_z[epoch][j][i])]), axis=0)\n",
    "                # evaluate the various conditional properties    \n",
    "                Esp_X_given_z[epoch][j] = np.mean(values, axis=0)\n",
    "                f_dec_z[epoch][j] = model(torch.tensor(Esp_X_given_z[epoch][j].astype('float32'))).detach().numpy()\n",
    "                Grad_Esp_X_given_z[epoch][j] = grad_xi_ae(model, Esp_X_given_z[epoch][j])\n",
    "                # first compute the standard deviation in x and y coordinates [axis=0 asks to loop over points X]\n",
    "                Std2_X_given_z[epoch][j] = np.std(values, axis=(0))\n",
    "                # we next sum the variances and take the square root\n",
    "                Std1_X_given_z[epoch][j] = np.sqrt( np.sum(Std2_X_given_z[epoch][j]**2) )\n",
    "                        \n",
    "    return model, training_data, X_given_z, z_bins, Esp_X_given_z, Grad_Esp_X_given_z, Std1_X_given_z, Std2_X_given_z, f_dec_z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0d4f30",
   "metadata": {},
   "source": [
    " ### Data set generation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ecc769",
   "metadata": {},
   "source": [
    "In the following box we generate a trajectory in the desired potential with the function previously defined. It will be further used for the AE training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267f0926",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameters \n",
    "beta = 2\n",
    "delta_t = 0.01\n",
    "T = 500000\n",
    "save = 10\n",
    "seed = None \n",
    "## Potential can be changed to whatever is defined above, make sure the otgher parameters are changed accordingly\n",
    "pot = TripleWellOneChannelPotential(beta) \n",
    "x_domain = [-2, 2] #we set it here for the coming plots\n",
    "y_domain = [-1.5, 2.5] #we set it here for the coming plots\n",
    "\n",
    "x_0 = np.array([0, 1])\n",
    "\n",
    "### Generate the trajectory\n",
    "trajectory, _ = UnbiasedTraj(pot, x_0, delta_t=delta_t, T=T, save=save, save_energy=False, seed=seed)\n",
    "\n",
    "\n",
    "###Compute the potential on the appropriate grid for nice plot\n",
    "gridx = np.linspace(x_domain[0], x_domain[1], 100)\n",
    "gridy = np.linspace(y_domain[0], y_domain[1], 100)\n",
    "x_plot = np.outer(gridx, np.ones(100)) \n",
    "y_plot = np.outer(gridy, np.ones(100)).T \n",
    "potential_on_grid = np.zeros([100, 100])\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        potential_on_grid[i, j] = pot.V(np.array([grid[i], grid[j]]))\n",
    "\n",
    "### Plot the trajectory \n",
    "fig = plt.figure(figsize=(9,3))\n",
    "ax0 = fig.add_subplot(1, 2, 1)\n",
    "ax1 = fig.add_subplot(1, 2, 2)\n",
    "ax0.pcolormesh(x_plot, y_plot, potential_on_grid, cmap='coolwarm_r', shading='auto')\n",
    "ax0.scatter(trajectory[:,0], trajectory[:,1], marker='x')\n",
    "ax1.plot(range(len(trajectory[:,0])), trajectory[:,0], label='x coodinate along trajectory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987afeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "num_epochs = 20\n",
    "learning_rate = 0.005\n",
    "n_bins_z = 20          # number of bins in the Encoded dimension\n",
    "optimizer_algo='Adam'  # Adam by default, otherwise SGD\n",
    "ae1 = SimpleAutoEncoder(2,1) \n",
    "print(ae1) \n",
    "\n",
    "\n",
    "#--- First training of the NN ---\n",
    "loss_function, optimizer = set_learning_parameters(ae1, learning_rate=learning_rate)\n",
    "(\n",
    "    ae1,\n",
    "    training_data1,\n",
    "    X_given_z,\n",
    "    z_bins,\n",
    "    Esp_X_given_z,\n",
    "    Grad_Esp_X_given_z,\n",
    "    Std1_X_given_z,\n",
    "    Std2_X_given_z,\n",
    "    f_dec_z\n",
    ") = train_with_variance_decomposition_plots(ae1,\n",
    "                                            loss_function,\n",
    "                                            optimizer,\n",
    "                                            trajectory,\n",
    "                                            np.ones(trajectory.shape[0]),\n",
    "                                            batch_size=batch_size,\n",
    "                                            num_epochs=num_epochs,\n",
    "                                            n_bins_z=n_bins_z,\n",
    "                                            x_domain=x_domain,\n",
    "                                            y_domain=y_domain\n",
    "                              )\n",
    "\n",
    "#--- Compute average losses per epoch ---\n",
    "loss_evol1 = []\n",
    "for i in range(len(training_data1)):\n",
    "    loss_evol1.append([torch.mean(training_data1[i][0]), torch.mean(training_data1[i][1])])\n",
    "loss_evol1 = np.array(loss_evol1)\n",
    "\n",
    "#--- Compute things to do \"nice\" plots ---\n",
    "# Ort_Grad_Esp_X_given_z is a unit tangent vector to the isoline \\xi = cst\n",
    "# computed by taking the normalized orthogonal to \\nabla \\xi\n",
    "Ort_Grad_Esp_X_given_z = [[] for i in range(num_epochs)]\n",
    "# Conditional average of X plus/minus 1.96*standard deviation in the tangent direction\n",
    "Esp_p_X_given_z = [[] for i in range(num_epochs)]\n",
    "Esp_m_X_given_z = [[] for i in range(num_epochs)]\n",
    "Var_Esp_X_given_z = np.zeros(num_epochs)\n",
    "for epoch in range(num_epochs):\n",
    "    j = 0\n",
    "    # loop over the lists over bins to eliminate empty bins\n",
    "    while j < len(z_bins[epoch][0]):\n",
    "        if len(X_given_z[epoch][j]) == 0:\n",
    "            z_bins[epoch][0].pop(j)\n",
    "            Esp_X_given_z[epoch].pop(j)\n",
    "            Grad_Esp_X_given_z[epoch].pop(j)\n",
    "            Std1_X_given_z[epoch].pop(j)\n",
    "            Std2_X_given_z[epoch].pop(j) \n",
    "            X_given_z[epoch].pop(j)\n",
    "            f_dec_z[epoch].pop(j)\n",
    "        else:\n",
    "            j += 1\n",
    "    for j in range(len(z_bins[epoch][0])):\n",
    "        # computation of the tangent vector orthogonal to \\nabla \\xi = (a_1,a_2), i.e. t = (-a_2,a_1)/normalization\n",
    "        Ort_Grad_Esp_X_given_z[epoch].append(np.zeros(2))\n",
    "        Ort_Grad_Esp_X_given_z[epoch][j][0] = - Grad_Esp_X_given_z[epoch][j][1] \n",
    "        Ort_Grad_Esp_X_given_z[epoch][j][1] = Grad_Esp_X_given_z[epoch][j][0] \n",
    "        Ort_Grad_Esp_X_given_z[epoch][j] = Ort_Grad_Esp_X_given_z[epoch][j] / np.sqrt(np.sum(Grad_Esp_X_given_z[epoch][j]**2))\n",
    "        # plotting average of X in a bin +/- standard deviation along tangent vector\n",
    "        Esp_m_X_given_z[epoch].append(Esp_X_given_z[epoch][j] - 1.96 * Std1_X_given_z[epoch][j] * Ort_Grad_Esp_X_given_z[epoch][j])\n",
    "        Esp_p_X_given_z[epoch].append(Esp_X_given_z[epoch][j] + 1.96 * Std1_X_given_z[epoch][j] * Ort_Grad_Esp_X_given_z[epoch][j])\n",
    "    Var_Esp_X_given_z[epoch] = np.std(np.array(Esp_X_given_z[epoch]))\n",
    "\n",
    "# Obtain values of the RC and potential on a grid to plot contour lines\n",
    "xi_ae1_on_grid = np.zeros([100, 100])\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        x = np.array([gridx[i], gridy[j]])\n",
    "        xi_ae1_on_grid[i,j] = xi_ae(ae1, x)\n",
    "\n",
    "#--- Plot the results ---\n",
    "start_epoch_index = 1\n",
    "fig, (ax0, ax1)  = plt.subplots(1,2, figsize=(10,4)) \n",
    "ax0.plot(loss_evol1[start_epoch_index:, 0], '--', label='train loss', marker='x')\n",
    "ax0.plot(range(1, len(loss_evol1[start_epoch_index:, 1])), loss_evol1[start_epoch_index: -1, 1], '-.', label='test loss', marker='+')\n",
    "ax0.legend()\n",
    "ax1.pcolormesh(x_plot, y_plot, potential_on_grid, cmap='coolwarm_r',shading='auto')\n",
    "ax1.contour(x_plot, y_plot, xi_ae1_on_grid, 20, cmap = 'viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2c398b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- variance of conditional averages ---\n",
    "plt.figure()\n",
    "plt.plot(range(num_epochs), Var_Esp_X_given_z)\n",
    "plt.title('Variance of conditional averages as a function of epochs')\n",
    "plt.xlabel('epoch number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49112398",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Conditionnal expectancy and decoded values ---\n",
    "index = -1\n",
    "plt.figure()\n",
    "plt.title('End of last epoch')\n",
    "plt.plot(np.array(Esp_X_given_z[index])[:, 0], np.array(Esp_X_given_z[index])[:, 1], '-o', color='b', label='cond. avg.')\n",
    "plt.plot(np.array(Esp_p_X_given_z[index])[:, 0], np.array(Esp_p_X_given_z[index])[:, 1], '-.+', color='b')\n",
    "plt.plot(np.array(Esp_m_X_given_z[index])[:, 0], np.array(Esp_m_X_given_z[index])[:, 1], '-.+', color='b')\n",
    "plt.plot(np.array(f_dec_z[index])[:, 0], np.array(f_dec_z[index])[:, 1], '*', color='black', label='decoder last epoch')\n",
    "plt.pcolormesh(x_plot, y_plot, potential_on_grid, cmap='coolwarm_r',shading='auto')\n",
    "index = 0\n",
    "plt.plot(np.array(f_dec_z[index])[:, 0], np.array(f_dec_z[index])[:, 1], '*', color='pink', label='decoder first epoch')\n",
    "plt.pcolormesh(x_plot, y_plot, potential_on_grid, cmap='coolwarm_r',shading='auto')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441dd4e1",
   "metadata": {},
   "source": [
    "We continue the training, decoder is clearly not optimal. We just copy the cell of the training with commenting the  line which initialises the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716e4ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "num_epochs = 20\n",
    "learning_rate = 0.005\n",
    "n_bins_z = 20          # number of bins in the Encoded dimension\n",
    "optimizer_algo='Adam'  # Adam by default, otherwise SGD\n",
    "#ae1 = SimpleAutoEncoder(2,1)  ### We don't want  to re-initialise the model here\n",
    "print(ae1) \n",
    "\n",
    "\n",
    "#--- First training of the NN ---\n",
    "loss_function, optimizer = set_learning_parameters(ae1, learning_rate=learning_rate)\n",
    "(\n",
    "    ae1,\n",
    "    training_data1,\n",
    "    X_given_z,\n",
    "    z_bins,\n",
    "    Esp_X_given_z,\n",
    "    Grad_Esp_X_given_z,\n",
    "    Std1_X_given_z,\n",
    "    Std2_X_given_z,\n",
    "    f_dec_z\n",
    ") = train_with_variance_decomposition_plots(ae1,\n",
    "                                            loss_function,\n",
    "                                            optimizer,\n",
    "                                            trajectory,\n",
    "                                            np.ones(trajectory.shape[0]),\n",
    "                                            batch_size=batch_size,\n",
    "                                            num_epochs=num_epochs,\n",
    "                                            n_bins_z=n_bins_z,\n",
    "                                            x_domain=x_domain,\n",
    "                                            y_domain=y_domain\n",
    "                              )\n",
    "\n",
    "#--- Compute average losses per epoch ---\n",
    "loss_evol1 = []\n",
    "for i in range(len(training_data1)):\n",
    "    loss_evol1.append([torch.mean(training_data1[i][0]), torch.mean(training_data1[i][1])])\n",
    "loss_evol1 = np.array(loss_evol1)\n",
    "\n",
    "#--- Compute things to do \"nice\" plots ---\n",
    "# Ort_Grad_Esp_X_given_z is a unit tangent vector to the isoline \\xi = cst\n",
    "# computed by taking the normalized orthogonal to \\nabla \\xi\n",
    "Ort_Grad_Esp_X_given_z = [[] for i in range(num_epochs)]\n",
    "# Conditional average of X plus/minus 1.96*standard deviation in the tangent direction\n",
    "Esp_p_X_given_z = [[] for i in range(num_epochs)]\n",
    "Esp_m_X_given_z = [[] for i in range(num_epochs)]\n",
    "Var_Esp_X_given_z = np.zeros(num_epochs)\n",
    "for epoch in range(num_epochs):\n",
    "    j = 0\n",
    "    # loop over the lists over bins to eliminate empty bins\n",
    "    while j < len(z_bins[epoch][0]):\n",
    "        if len(X_given_z[epoch][j]) == 0:\n",
    "            z_bins[epoch][0].pop(j)\n",
    "            Esp_X_given_z[epoch].pop(j)\n",
    "            Grad_Esp_X_given_z[epoch].pop(j)\n",
    "            Std1_X_given_z[epoch].pop(j)\n",
    "            Std2_X_given_z[epoch].pop(j) \n",
    "            X_given_z[epoch].pop(j)\n",
    "            f_dec_z[epoch].pop(j)\n",
    "        else:\n",
    "            j += 1\n",
    "    for j in range(len(z_bins[epoch][0])):\n",
    "        # computation of the tangent vector orthogonal to \\nabla \\xi = (a_1,a_2), i.e. t = (-a_2,a_1)/normalization\n",
    "        Ort_Grad_Esp_X_given_z[epoch].append(np.zeros(2))\n",
    "        Ort_Grad_Esp_X_given_z[epoch][j][0] = - Grad_Esp_X_given_z[epoch][j][1] \n",
    "        Ort_Grad_Esp_X_given_z[epoch][j][1] = Grad_Esp_X_given_z[epoch][j][0] \n",
    "        Ort_Grad_Esp_X_given_z[epoch][j] = Ort_Grad_Esp_X_given_z[epoch][j] / np.sqrt(np.sum(Grad_Esp_X_given_z[epoch][j]**2))\n",
    "        # plotting average of X in a bin +/- standard deviation along tangent vector\n",
    "        Esp_m_X_given_z[epoch].append(Esp_X_given_z[epoch][j] - 1.96 * Std1_X_given_z[epoch][j] * Ort_Grad_Esp_X_given_z[epoch][j])\n",
    "        Esp_p_X_given_z[epoch].append(Esp_X_given_z[epoch][j] + 1.96 * Std1_X_given_z[epoch][j] * Ort_Grad_Esp_X_given_z[epoch][j])\n",
    "    Var_Esp_X_given_z[epoch] = np.std(np.array(Esp_X_given_z[epoch]))\n",
    "\n",
    "# Obtain values of the RC and potential on a grid to plot contour lines\n",
    "xi_ae1_on_grid = np.zeros([100, 100])\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        x = np.array([gridx[i], gridy[j]])\n",
    "        xi_ae1_on_grid[i,j] = xi_ae(ae1, x)\n",
    "\n",
    "#--- Plot the results ---\n",
    "start_epoch_index = 1\n",
    "fig, (ax0, ax1)  = plt.subplots(1,2, figsize=(10,4)) \n",
    "ax0.plot(loss_evol1[start_epoch_index:, 0], '--', label='train loss', marker='x')\n",
    "ax0.plot(range(1, len(loss_evol1[start_epoch_index:, 1])), loss_evol1[start_epoch_index: -1, 1], '-.', label='test loss', marker='+')\n",
    "ax0.legend()\n",
    "ax1.pcolormesh(x_plot, y_plot, potential_on_grid, cmap='coolwarm_r',shading='auto')\n",
    "ax1.contour(x_plot, y_plot, xi_ae1_on_grid, 40, cmap = 'viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a6fb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- variance of conditional averages ---\n",
    "plt.figure()\n",
    "plt.plot(range(num_epochs), Var_Esp_X_given_z)\n",
    "plt.title('Variance of conditional averages as a function of epochs')\n",
    "plt.xlabel('epoch number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cf1aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Conditionnal expectancy and decoded values ---\n",
    "index = -1\n",
    "plt.figure()\n",
    "plt.title('End of last epoch')\n",
    "plt.plot(np.array(Esp_X_given_z[index])[:, 0], np.array(Esp_X_given_z[index])[:, 1], '-o', color='b', label='cond. avg.')\n",
    "plt.plot(np.array(Esp_p_X_given_z[index])[:, 0], np.array(Esp_p_X_given_z[index])[:, 1], '-.+', color='b')\n",
    "plt.plot(np.array(Esp_m_X_given_z[index])[:, 0], np.array(Esp_m_X_given_z[index])[:, 1], '-.+', color='b')\n",
    "plt.plot(np.array(f_dec_z[index])[:, 0], np.array(f_dec_z[index])[:, 1], '*', color='black', label='decoder last epoch')\n",
    "plt.pcolormesh(x_plot, y_plot, potential_on_grid, cmap='coolwarm_r',shading='auto')\n",
    "index = 0\n",
    "plt.plot(np.array(f_dec_z[index])[:, 0], np.array(f_dec_z[index])[:, 1], '*', color='pink', label='decoder first epoch')\n",
    "plt.pcolormesh(x_plot, y_plot, potential_on_grid, cmap='coolwarm_r',shading='auto')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af977e4",
   "metadata": {},
   "source": [
    "This looks better but we a few more epoch would be good to improve it a bit. Here we are limited by the architechture of the AE as the decoder is just an affine function $\\mathbb{R} \\ to \\mathbb{R}²$ so the decoded values must be on a line in the x,y plane so let's try something with a more complex architechture only in the decoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95da4863",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "num_epochs = 10\n",
    "learning_rate = 0.005\n",
    "n_bins_z = 20          # number of bins in the Encoded dimension\n",
    "optimizer_algo='Adam'  # Adam by default, otherwise SGD\n",
    "ae1 = AssymmetricAutoEncoder() ### We want  to re-initialise the model here see comment below\n",
    "print(ae1) \n",
    "\n",
    "\n",
    "#--- First training of the NN ---\n",
    "loss_function, optimizer = set_learning_parameters(ae1, learning_rate=learning_rate)\n",
    "(\n",
    "    ae1,\n",
    "    training_data1,\n",
    "    X_given_z,\n",
    "    z_bins,\n",
    "    Esp_X_given_z,\n",
    "    Grad_Esp_X_given_z,\n",
    "    Std1_X_given_z,\n",
    "    Std2_X_given_z,\n",
    "    f_dec_z\n",
    ") = train_with_variance_decomposition_plots(ae1,\n",
    "                                            loss_function,\n",
    "                                            optimizer,\n",
    "                                            trajectory,\n",
    "                                            np.ones(trajectory.shape[0]),\n",
    "                                            batch_size=batch_size,\n",
    "                                            num_epochs=num_epochs,\n",
    "                                            n_bins_z=n_bins_z,\n",
    "                                            x_domain=x_domain,\n",
    "                                            y_domain=y_domain\n",
    "                              )\n",
    "\n",
    "#--- Compute average losses per epoch ---\n",
    "loss_evol1 = []\n",
    "for i in range(len(training_data1)):\n",
    "    loss_evol1.append([torch.mean(training_data1[i][0]), torch.mean(training_data1[i][1])])\n",
    "loss_evol1 = np.array(loss_evol1)\n",
    "\n",
    "#--- Compute things to do \"nice\" plots ---\n",
    "# Ort_Grad_Esp_X_given_z is a unit tangent vector to the isoline \\xi = cst\n",
    "# computed by taking the normalized orthogonal to \\nabla \\xi\n",
    "Ort_Grad_Esp_X_given_z = [[] for i in range(num_epochs)]\n",
    "# Conditional average of X plus/minus 1.96*standard deviation in the tangent direction\n",
    "Esp_p_X_given_z = [[] for i in range(num_epochs)]\n",
    "Esp_m_X_given_z = [[] for i in range(num_epochs)]\n",
    "Var_Esp_X_given_z = np.zeros(num_epochs)\n",
    "for epoch in range(num_epochs):\n",
    "    j = 0\n",
    "    # loop over the lists over bins to eliminate empty bins\n",
    "    while j < len(z_bins[epoch][0]):\n",
    "        if len(X_given_z[epoch][j]) == 0:\n",
    "            z_bins[epoch][0].pop(j)\n",
    "            Esp_X_given_z[epoch].pop(j)\n",
    "            Grad_Esp_X_given_z[epoch].pop(j)\n",
    "            Std1_X_given_z[epoch].pop(j)\n",
    "            Std2_X_given_z[epoch].pop(j) \n",
    "            X_given_z[epoch].pop(j)\n",
    "            f_dec_z[epoch].pop(j)\n",
    "        else:\n",
    "            j += 1\n",
    "    for j in range(len(z_bins[epoch][0])):\n",
    "        # computation of the tangent vector orthogonal to \\nabla \\xi = (a_1,a_2), i.e. t = (-a_2,a_1)/normalization\n",
    "        Ort_Grad_Esp_X_given_z[epoch].append(np.zeros(2))\n",
    "        Ort_Grad_Esp_X_given_z[epoch][j][0] = - Grad_Esp_X_given_z[epoch][j][1] \n",
    "        Ort_Grad_Esp_X_given_z[epoch][j][1] = Grad_Esp_X_given_z[epoch][j][0] \n",
    "        Ort_Grad_Esp_X_given_z[epoch][j] = Ort_Grad_Esp_X_given_z[epoch][j] / np.sqrt(np.sum(Grad_Esp_X_given_z[epoch][j]**2))\n",
    "        # plotting average of X in a bin +/- standard deviation along tangent vector\n",
    "        Esp_m_X_given_z[epoch].append(Esp_X_given_z[epoch][j] - 1.96 * Std1_X_given_z[epoch][j] * Ort_Grad_Esp_X_given_z[epoch][j])\n",
    "        Esp_p_X_given_z[epoch].append(Esp_X_given_z[epoch][j] + 1.96 * Std1_X_given_z[epoch][j] * Ort_Grad_Esp_X_given_z[epoch][j])\n",
    "    Var_Esp_X_given_z[epoch] = np.std(np.array(Esp_X_given_z[epoch]))\n",
    "\n",
    "# Obtain values of the RC and potential on a grid to plot contour lines\n",
    "xi_ae1_on_grid = np.zeros([100, 100])\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        x = np.array([gridx[i], gridy[j]])\n",
    "        xi_ae1_on_grid[i,j] = xi_ae(ae1, x)\n",
    "\n",
    "#--- Plot the results ---\n",
    "start_epoch_index = 1\n",
    "fig, (ax0, ax1)  = plt.subplots(1,2, figsize=(10,4)) \n",
    "ax0.plot(loss_evol1[start_epoch_index:, 0], '--', label='train loss', marker='x')\n",
    "ax0.plot(range(1, len(loss_evol1[start_epoch_index:, 1])), loss_evol1[start_epoch_index: -1, 1], '-.', label='test loss', marker='+')\n",
    "ax0.legend()\n",
    "ax1.pcolormesh(x_plot, y_plot, potential_on_grid, cmap='coolwarm_r',shading='auto')\n",
    "ax1.contour(x_plot, y_plot, xi_ae1_on_grid, 40, cmap = 'viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a767f6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- variance of conditional averages ---\n",
    "plt.figure()\n",
    "plt.plot(range(num_epochs), Var_Esp_X_given_z)\n",
    "plt.title('Variance of conditional averages as a function of epochs')\n",
    "plt.xlabel('epoch number')\n",
    "\n",
    "#--- Conditionnal expectancy and decoded values ---\n",
    "index = -1\n",
    "plt.figure()\n",
    "plt.title('End of last epoch')\n",
    "plt.plot(np.array(Esp_X_given_z[index])[:, 0], np.array(Esp_X_given_z[index])[:, 1], '-o', color='b', label='cond. avg.')\n",
    "plt.plot(np.array(Esp_p_X_given_z[index])[:, 0], np.array(Esp_p_X_given_z[index])[:, 1], '-.+', color='b')\n",
    "plt.plot(np.array(Esp_m_X_given_z[index])[:, 0], np.array(Esp_m_X_given_z[index])[:, 1], '-.+', color='b')\n",
    "plt.plot(np.array(f_dec_z[index])[:, 0], np.array(f_dec_z[index])[:, 1], '*', color='black', label='decoder last epoch')\n",
    "plt.pcolormesh(x_plot, y_plot, potential_on_grid, cmap='coolwarm_r',shading='auto')\n",
    "index = 0\n",
    "plt.plot(np.array(f_dec_z[index])[:, 0], np.array(f_dec_z[index])[:, 1], '*', color='pink', label='decoder first epoch')\n",
    "plt.pcolormesh(x_plot, y_plot, potential_on_grid, cmap='coolwarm_r',shading='auto')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e709615",
   "metadata": {},
   "source": [
    "Decoder is still not optimal, let's do more epochs to see if something happens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa02dd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "num_epochs = 50\n",
    "learning_rate = 0.005\n",
    "n_bins_z = 20          # number of bins in the Encoded dimension\n",
    "optimizer_algo='Adam'  # Adam by default, otherwise SGD\n",
    "## ae1 = AssymmetricAutoEncoder() ### We don't want  to re-initialise the model here\n",
    "print(ae1) \n",
    "\n",
    "\n",
    "#--- First training of the NN ---\n",
    "loss_function, optimizer = set_learning_parameters(ae1, learning_rate=learning_rate)\n",
    "(\n",
    "    ae1,\n",
    "    training_data1,\n",
    "    X_given_z,\n",
    "    z_bins,\n",
    "    Esp_X_given_z,\n",
    "    Grad_Esp_X_given_z,\n",
    "    Std1_X_given_z,\n",
    "    Std2_X_given_z,\n",
    "    f_dec_z\n",
    ") = train_with_variance_decomposition_plots(ae1,\n",
    "                                            loss_function,\n",
    "                                            optimizer,\n",
    "                                            trajectory,\n",
    "                                            np.ones(trajectory.shape[0]),\n",
    "                                            batch_size=batch_size,\n",
    "                                            num_epochs=num_epochs,\n",
    "                                            n_bins_z=n_bins_z,\n",
    "                                            x_domain=x_domain,\n",
    "                                            y_domain=y_domain\n",
    "                              )\n",
    "\n",
    "#--- Compute average losses per epoch ---\n",
    "loss_evol1 = []\n",
    "for i in range(len(training_data1)):\n",
    "    loss_evol1.append([torch.mean(training_data1[i][0]), torch.mean(training_data1[i][1])])\n",
    "loss_evol1 = np.array(loss_evol1)\n",
    "\n",
    "#--- Compute things to do \"nice\" plots ---\n",
    "# Ort_Grad_Esp_X_given_z is a unit tangent vector to the isoline \\xi = cst\n",
    "# computed by taking the normalized orthogonal to \\nabla \\xi\n",
    "Ort_Grad_Esp_X_given_z = [[] for i in range(num_epochs)]\n",
    "# Conditional average of X plus/minus 1.96*standard deviation in the tangent direction\n",
    "Esp_p_X_given_z = [[] for i in range(num_epochs)]\n",
    "Esp_m_X_given_z = [[] for i in range(num_epochs)]\n",
    "Var_Esp_X_given_z = np.zeros(num_epochs)\n",
    "for epoch in range(num_epochs):\n",
    "    j = 0\n",
    "    # loop over the lists over bins to eliminate empty bins\n",
    "    while j < len(z_bins[epoch][0]):\n",
    "        if len(X_given_z[epoch][j]) == 0:\n",
    "            z_bins[epoch][0].pop(j)\n",
    "            Esp_X_given_z[epoch].pop(j)\n",
    "            Grad_Esp_X_given_z[epoch].pop(j)\n",
    "            Std1_X_given_z[epoch].pop(j)\n",
    "            Std2_X_given_z[epoch].pop(j) \n",
    "            X_given_z[epoch].pop(j)\n",
    "            f_dec_z[epoch].pop(j)\n",
    "        else:\n",
    "            j += 1\n",
    "    for j in range(len(z_bins[epoch][0])):\n",
    "        # computation of the tangent vector orthogonal to \\nabla \\xi = (a_1,a_2), i.e. t = (-a_2,a_1)/normalization\n",
    "        Ort_Grad_Esp_X_given_z[epoch].append(np.zeros(2))\n",
    "        Ort_Grad_Esp_X_given_z[epoch][j][0] = - Grad_Esp_X_given_z[epoch][j][1] \n",
    "        Ort_Grad_Esp_X_given_z[epoch][j][1] = Grad_Esp_X_given_z[epoch][j][0] \n",
    "        Ort_Grad_Esp_X_given_z[epoch][j] = Ort_Grad_Esp_X_given_z[epoch][j] / np.sqrt(np.sum(Grad_Esp_X_given_z[epoch][j]**2))\n",
    "        # plotting average of X in a bin +/- standard deviation along tangent vector\n",
    "        Esp_m_X_given_z[epoch].append(Esp_X_given_z[epoch][j] - 1.96 * Std1_X_given_z[epoch][j] * Ort_Grad_Esp_X_given_z[epoch][j])\n",
    "        Esp_p_X_given_z[epoch].append(Esp_X_given_z[epoch][j] + 1.96 * Std1_X_given_z[epoch][j] * Ort_Grad_Esp_X_given_z[epoch][j])\n",
    "    Var_Esp_X_given_z[epoch] = np.std(np.array(Esp_X_given_z[epoch]))\n",
    "\n",
    "# Obtain values of the RC and potential on a grid to plot contour lines\n",
    "xi_ae1_on_grid = np.zeros([100, 100])\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        x = np.array([gridx[i], gridy[j]])\n",
    "        xi_ae1_on_grid[i,j] = xi_ae(ae1, x)\n",
    "\n",
    "#--- Plot the results ---\n",
    "start_epoch_index = 1\n",
    "fig, (ax0, ax1)  = plt.subplots(1,2, figsize=(10,4)) \n",
    "ax0.plot(loss_evol1[start_epoch_index:, 0], '--', label='train loss', marker='x')\n",
    "ax0.plot(range(1, len(loss_evol1[start_epoch_index:, 1])), loss_evol1[start_epoch_index: -1, 1], '-.', label='test loss', marker='+')\n",
    "ax0.legend()\n",
    "ax1.pcolormesh(x_plot, y_plot, potential_on_grid, cmap='coolwarm_r',shading='auto')\n",
    "ax1.contour(x_plot, y_plot, xi_ae1_on_grid, 40, cmap = 'viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c808ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- variance of conditional averages ---\n",
    "plt.figure()\n",
    "plt.plot(range(num_epochs), Var_Esp_X_given_z)\n",
    "plt.title('Variance of conditional averages as a function of epochs')\n",
    "plt.xlabel('epoch number')\n",
    "\n",
    "#--- Conditionnal expectancy and decoded values ---\n",
    "index = -1\n",
    "plt.figure()\n",
    "plt.title('End of last epoch')\n",
    "plt.plot(np.array(Esp_X_given_z[index])[:, 0], np.array(Esp_X_given_z[index])[:, 1], '-o', color='b', label='cond. avg.')\n",
    "plt.plot(np.array(Esp_p_X_given_z[index])[:, 0], np.array(Esp_p_X_given_z[index])[:, 1], '-.+', color='b')\n",
    "plt.plot(np.array(Esp_m_X_given_z[index])[:, 0], np.array(Esp_m_X_given_z[index])[:, 1], '-.+', color='b')\n",
    "plt.plot(np.array(f_dec_z[index])[:, 0], np.array(f_dec_z[index])[:, 1], '*', color='black', label='decoder last epoch')\n",
    "plt.pcolormesh(x_plot, y_plot, potential_on_grid, cmap='coolwarm_r',shading='auto')\n",
    "index = 0\n",
    "plt.plot(np.array(f_dec_z[index])[:, 0], np.array(f_dec_z[index])[:, 1], '*', color='pink', label='decoder first epoch')\n",
    "plt.pcolormesh(x_plot, y_plot, potential_on_grid, cmap='coolwarm_r',shading='auto')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44307cdf",
   "metadata": {},
   "source": [
    "It seems to be learning something but supper slowly. As the points in the middle part are extremely in minority compared to those in each well, it learns there slowly. I think this behaviour will always show up for transition channels which are numerous times less probable that the \"states weels\". \n",
    "\n",
    "Let's try first to continue the training on only the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5b2580",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "n_bins_z = 20          # number of bins in the Encoded dimension\n",
    "optimizer_algo='Adam'  # Adam by default, otherwise SGD\n",
    " # ae1 = AssymmetricAutoEncoder() ### We don't want  to re-initialise the model here\n",
    "print(ae1) \n",
    "\n",
    "\n",
    "#--- First training of the NN ---\n",
    "loss_function, optimizer = set_learning_parameters(ae1.decoder, learning_rate=learning_rate) ## here we only give the decoder as model to the optimizer.\n",
    "(\n",
    "    ae1,\n",
    "    training_data1,\n",
    "    X_given_z,\n",
    "    z_bins,\n",
    "    Esp_X_given_z,\n",
    "    Grad_Esp_X_given_z,\n",
    "    Std1_X_given_z,\n",
    "    Std2_X_given_z,\n",
    "    f_dec_z\n",
    ") = train_with_variance_decomposition_plots(ae1,\n",
    "                                            loss_function,\n",
    "                                            optimizer,\n",
    "                                            trajectory,\n",
    "                                            np.ones(trajectory.shape[0]),\n",
    "                                            batch_size=batch_size,\n",
    "                                            num_epochs=num_epochs,\n",
    "                                            n_bins_z=n_bins_z,\n",
    "                                            x_domain=x_domain,\n",
    "                                            y_domain=y_domain\n",
    "                              )\n",
    "\n",
    "#--- Compute average losses per epoch ---\n",
    "loss_evol1 = []\n",
    "for i in range(len(training_data1)):\n",
    "    loss_evol1.append([torch.mean(training_data1[i][0]), torch.mean(training_data1[i][1])])\n",
    "loss_evol1 = np.array(loss_evol1)\n",
    "\n",
    "#--- Compute things to do \"nice\" plots ---\n",
    "# Ort_Grad_Esp_X_given_z is a unit tangent vector to the isoline \\xi = cst\n",
    "# computed by taking the normalized orthogonal to \\nabla \\xi\n",
    "Ort_Grad_Esp_X_given_z = [[] for i in range(num_epochs)]\n",
    "# Conditional average of X plus/minus 1.96*standard deviation in the tangent direction\n",
    "Esp_p_X_given_z = [[] for i in range(num_epochs)]\n",
    "Esp_m_X_given_z = [[] for i in range(num_epochs)]\n",
    "Var_Esp_X_given_z = np.zeros(num_epochs)\n",
    "for epoch in range(num_epochs):\n",
    "    j = 0\n",
    "    # loop over the lists over bins to eliminate empty bins\n",
    "    while j < len(z_bins[epoch][0]):\n",
    "        if len(X_given_z[epoch][j]) == 0:\n",
    "            z_bins[epoch][0].pop(j)\n",
    "            Esp_X_given_z[epoch].pop(j)\n",
    "            Grad_Esp_X_given_z[epoch].pop(j)\n",
    "            Std1_X_given_z[epoch].pop(j)\n",
    "            Std2_X_given_z[epoch].pop(j) \n",
    "            X_given_z[epoch].pop(j)\n",
    "            f_dec_z[epoch].pop(j)\n",
    "        else:\n",
    "            j += 1\n",
    "    for j in range(len(z_bins[epoch][0])):\n",
    "        # computation of the tangent vector orthogonal to \\nabla \\xi = (a_1,a_2), i.e. t = (-a_2,a_1)/normalization\n",
    "        Ort_Grad_Esp_X_given_z[epoch].append(np.zeros(2))\n",
    "        Ort_Grad_Esp_X_given_z[epoch][j][0] = - Grad_Esp_X_given_z[epoch][j][1] \n",
    "        Ort_Grad_Esp_X_given_z[epoch][j][1] = Grad_Esp_X_given_z[epoch][j][0] \n",
    "        Ort_Grad_Esp_X_given_z[epoch][j] = Ort_Grad_Esp_X_given_z[epoch][j] / np.sqrt(np.sum(Grad_Esp_X_given_z[epoch][j]**2))\n",
    "        # plotting average of X in a bin +/- standard deviation along tangent vector\n",
    "        Esp_m_X_given_z[epoch].append(Esp_X_given_z[epoch][j] - 1.96 * Std1_X_given_z[epoch][j] * Ort_Grad_Esp_X_given_z[epoch][j])\n",
    "        Esp_p_X_given_z[epoch].append(Esp_X_given_z[epoch][j] + 1.96 * Std1_X_given_z[epoch][j] * Ort_Grad_Esp_X_given_z[epoch][j])\n",
    "    Var_Esp_X_given_z[epoch] = np.std(np.array(Esp_X_given_z[epoch]))\n",
    "\n",
    "# Obtain values of the RC and potential on a grid to plot contour lines\n",
    "xi_ae1_on_grid = np.zeros([100, 100])\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        x = np.array([gridx[i], gridy[j]])\n",
    "        xi_ae1_on_grid[i,j] = xi_ae(ae1, x)\n",
    "\n",
    "#--- Plot the results ---\n",
    "start_epoch_index = 1\n",
    "fig, (ax0, ax1)  = plt.subplots(1,2, figsize=(10,4)) \n",
    "ax0.plot(loss_evol1[start_epoch_index:, 0], '--', label='train loss', marker='x')\n",
    "ax0.plot(range(1, len(loss_evol1[start_epoch_index:, 1])), loss_evol1[start_epoch_index: -1, 1], '-.', label='test loss', marker='+')\n",
    "ax0.legend()\n",
    "ax1.pcolormesh(x_plot, y_plot, potential_on_grid, cmap='coolwarm_r',shading='auto')\n",
    "ax1.contour(x_plot, y_plot, xi_ae1_on_grid, 40, cmap = 'viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb122144",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- variance of conditional averages ---\n",
    "plt.figure()\n",
    "plt.plot(range(num_epochs), Var_Esp_X_given_z)\n",
    "plt.title('Variance of conditional averages as a function of epochs')\n",
    "plt.xlabel('epoch number')\n",
    "## We should have something constant here as the encoder is supposed to stay the same.\n",
    "\n",
    "\n",
    "#--- Conditionnal expectancy and decoded values ---\n",
    "index = -1\n",
    "plt.figure()\n",
    "plt.title('End of last epoch')\n",
    "plt.plot(np.array(Esp_X_given_z[index])[:, 0], np.array(Esp_X_given_z[index])[:, 1], '-o', color='b', label='cond. avg.')\n",
    "plt.plot(np.array(Esp_p_X_given_z[index])[:, 0], np.array(Esp_p_X_given_z[index])[:, 1], '-.+', color='b')\n",
    "plt.plot(np.array(Esp_m_X_given_z[index])[:, 0], np.array(Esp_m_X_given_z[index])[:, 1], '-.+', color='b')\n",
    "plt.plot(np.array(f_dec_z[index])[:, 0], np.array(f_dec_z[index])[:, 1], '*', color='black', label='decoder last epoch')\n",
    "plt.pcolormesh(x_plot, y_plot, potential_on_grid, cmap='coolwarm_r',shading='auto')\n",
    "index = 0\n",
    "plt.plot(np.array(f_dec_z[index])[:, 0], np.array(f_dec_z[index])[:, 1], '*', color='pink', label='decoder first epoch')\n",
    "plt.pcolormesh(x_plot, y_plot, potential_on_grid, cmap='coolwarm_r',shading='auto')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723ac759",
   "metadata": {},
   "source": [
    "Let's see with more complex encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42b5a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "num_epochs = 50\n",
    "learning_rate = 0.005\n",
    "n_bins_z = 20          # number of bins in the Encoded dimension\n",
    "optimizer_algo='Adam'  # Adam by default, otherwise SGD\n",
    "ae1 = DeepAutoEncoder(2, [3, 2], 1) ### We don't want  to re-initialise the model here\n",
    "print(ae1) \n",
    "\n",
    "\n",
    "#--- First training of the NN ---\n",
    "loss_function, optimizer = set_learning_parameters(ae1, learning_rate=learning_rate) ## here we only give the decoder as model to the optimizer.\n",
    "(\n",
    "    ae1,\n",
    "    training_data1,\n",
    "    X_given_z,\n",
    "    z_bins,\n",
    "    Esp_X_given_z,\n",
    "    Grad_Esp_X_given_z,\n",
    "    Std1_X_given_z,\n",
    "    Std2_X_given_z,\n",
    "    f_dec_z\n",
    ") = train_with_variance_decomposition_plots(ae1,\n",
    "                                            loss_function,\n",
    "                                            optimizer,\n",
    "                                            trajectory,\n",
    "                                            np.ones(trajectory.shape[0]),\n",
    "                                            batch_size=batch_size,\n",
    "                                            num_epochs=num_epochs,\n",
    "                                            n_bins_z=n_bins_z,\n",
    "                                            x_domain=x_domain,\n",
    "                                            y_domain=y_domain\n",
    "                              )\n",
    "\n",
    "#--- Compute average losses per epoch ---\n",
    "loss_evol1 = []\n",
    "for i in range(len(training_data1)):\n",
    "    loss_evol1.append([torch.mean(training_data1[i][0]), torch.mean(training_data1[i][1])])\n",
    "loss_evol1 = np.array(loss_evol1)\n",
    "\n",
    "#--- Compute things to do \"nice\" plots ---\n",
    "# Ort_Grad_Esp_X_given_z is a unit tangent vector to the isoline \\xi = cst\n",
    "# computed by taking the normalized orthogonal to \\nabla \\xi\n",
    "Ort_Grad_Esp_X_given_z = [[] for i in range(num_epochs)]\n",
    "# Conditional average of X plus/minus 1.96*standard deviation in the tangent direction\n",
    "Esp_p_X_given_z = [[] for i in range(num_epochs)]\n",
    "Esp_m_X_given_z = [[] for i in range(num_epochs)]\n",
    "Var_Esp_X_given_z = np.zeros(num_epochs)\n",
    "for epoch in range(num_epochs):\n",
    "    j = 0\n",
    "    # loop over the lists over bins to eliminate empty bins\n",
    "    while j < len(z_bins[epoch][0]):\n",
    "        if len(X_given_z[epoch][j]) == 0:\n",
    "            z_bins[epoch][0].pop(j)\n",
    "            Esp_X_given_z[epoch].pop(j)\n",
    "            Grad_Esp_X_given_z[epoch].pop(j)\n",
    "            Std1_X_given_z[epoch].pop(j)\n",
    "            Std2_X_given_z[epoch].pop(j) \n",
    "            X_given_z[epoch].pop(j)\n",
    "            f_dec_z[epoch].pop(j)\n",
    "        else:\n",
    "            j += 1\n",
    "    for j in range(len(z_bins[epoch][0])):\n",
    "        # computation of the tangent vector orthogonal to \\nabla \\xi = (a_1,a_2), i.e. t = (-a_2,a_1)/normalization\n",
    "        Ort_Grad_Esp_X_given_z[epoch].append(np.zeros(2))\n",
    "        Ort_Grad_Esp_X_given_z[epoch][j][0] = - Grad_Esp_X_given_z[epoch][j][1] \n",
    "        Ort_Grad_Esp_X_given_z[epoch][j][1] = Grad_Esp_X_given_z[epoch][j][0] \n",
    "        Ort_Grad_Esp_X_given_z[epoch][j] = Ort_Grad_Esp_X_given_z[epoch][j] / np.sqrt(np.sum(Grad_Esp_X_given_z[epoch][j]**2))\n",
    "        # plotting average of X in a bin +/- standard deviation along tangent vector\n",
    "        Esp_m_X_given_z[epoch].append(Esp_X_given_z[epoch][j] - 1.96 * Std1_X_given_z[epoch][j] * Ort_Grad_Esp_X_given_z[epoch][j])\n",
    "        Esp_p_X_given_z[epoch].append(Esp_X_given_z[epoch][j] + 1.96 * Std1_X_given_z[epoch][j] * Ort_Grad_Esp_X_given_z[epoch][j])\n",
    "    Var_Esp_X_given_z[epoch] = np.std(np.array(Esp_X_given_z[epoch]))\n",
    "\n",
    "# Obtain values of the RC and potential on a grid to plot contour lines\n",
    "xi_ae1_on_grid = np.zeros([100, 100])\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        x = np.array([gridx[i], gridy[j]])\n",
    "        xi_ae1_on_grid[i,j] = xi_ae(ae1, x)\n",
    "\n",
    "#--- Plot the results ---\n",
    "start_epoch_index = 1\n",
    "fig, (ax0, ax1)  = plt.subplots(1,2, figsize=(10,4)) \n",
    "ax0.plot(loss_evol1[start_epoch_index:, 0], '--', label='train loss', marker='x')\n",
    "ax0.plot(range(1, len(loss_evol1[start_epoch_index:, 1])), loss_evol1[start_epoch_index: -1, 1], '-.', label='test loss', marker='+')\n",
    "ax0.legend()\n",
    "ax1.pcolormesh(x_plot, y_plot, potential_on_grid, cmap='coolwarm_r',shading='auto')\n",
    "ax1.contour(x_plot, y_plot, xi_ae1_on_grid, 40, cmap = 'viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2343b7a",
   "metadata": {},
   "source": [
    "I would say that the encoder overfitted with the data, it should learn something symmetrical with respect t the y axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77555259",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- variance of conditional averages ---\n",
    "plt.figure()\n",
    "plt.plot(range(num_epochs), Var_Esp_X_given_z)\n",
    "plt.title('Variance of conditional averages as a function of epochs')\n",
    "plt.xlabel('epoch number')\n",
    "## We should have something constant here as the encoder is supposed to stay the same.\n",
    "\n",
    "\n",
    "#--- Conditionnal expectancy and decoded values ---\n",
    "index = -1\n",
    "plt.figure()\n",
    "plt.title('End of last epoch')\n",
    "plt.plot(np.array(Esp_X_given_z[index])[:, 0], np.array(Esp_X_given_z[index])[:, 1], '-o', color='b', label='cond. avg.')\n",
    "plt.plot(np.array(Esp_p_X_given_z[index])[:, 0], np.array(Esp_p_X_given_z[index])[:, 1], '-.+', color='b')\n",
    "plt.plot(np.array(Esp_m_X_given_z[index])[:, 0], np.array(Esp_m_X_given_z[index])[:, 1], '-.+', color='b')\n",
    "plt.plot(np.array(f_dec_z[index])[:, 0], np.array(f_dec_z[index])[:, 1], '*', color='black', label='decoder last epoch')\n",
    "plt.pcolormesh(x_plot, y_plot, potential_on_grid, cmap='coolwarm_r',shading='auto')\n",
    "index = 0\n",
    "plt.plot(np.array(f_dec_z[index])[:, 0], np.array(f_dec_z[index])[:, 1], '*', color='pink', label='decoder first epoch')\n",
    "plt.pcolormesh(x_plot, y_plot, potential_on_grid, cmap='coolwarm_r',shading='auto')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1074de87",
   "metadata": {},
   "source": [
    "Maybe the decoder is too simple. Let's try something even deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02277bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepAutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, bottleneck_dim):\n",
    "        \"\"\"Initialise auto encoder with hyperbolic tangent activation function\n",
    "        You can uncomment certain lines in the encoder and decoder functions to modify the topology of the network\n",
    "        Make sure when you initialise the AE object that the list 'hidden_dims' has a length consistent with the architecture\n",
    "\n",
    "        :param input_dim: int, Number of dimension of the input vectors\n",
    "        :param hidden_dims: list, List of hidden layers\n",
    "        :param bottleneck_dim: int, Number of dimension of the bottleneck\n",
    "        \"\"\"\n",
    "        super(DeepAutoEncoder, self).__init__()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, hidden_dims[0]),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(hidden_dims[0], hidden_dims[1]),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(hidden_dims[1], hidden_dims[2]),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(hidden_dims[2], hidden_dims[3]),\n",
    "            torch.nn.Tanh(),\n",
    "            # torch.nn.Linear(hidden_dims[3], hidden_dims[4]),\n",
    "            # torch.nn.Tanh(),\n",
    "            torch.nn.Linear(hidden_dims[-1], bottleneck_dim),\n",
    "            torch.nn.Tanh()\n",
    "        )\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(bottleneck_dim, hidden_dims[-1]),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(hidden_dims[-1], hidden_dims[-2]),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(hidden_dims[-2], hidden_dims[-3]),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(hidden_dims[-3], hidden_dims[-4]),\n",
    "            torch.nn.Tanh(),\n",
    "            # torch.nn.Linear(hidden_dims[-4], hidden_dims[-5]),\n",
    "            # torch.nn.Tanh(),\n",
    "            torch.nn.Linear(hidden_dims[0], input_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        # Input Linear function\n",
    "        encoded = self.encoder(inp)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec9e739",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "num_epochs = 40\n",
    "learning_rate = 0.005\n",
    "n_bins_z = 20          # number of bins in the Encoded dimension\n",
    "optimizer_algo='Adam'  # Adam by default, otherwise SGD\n",
    "ae1 = DeepAutoEncoder(2, [3, 4, 3, 2], 1) ### We want  to re-initialise the model here\n",
    "print(ae1) \n",
    "\n",
    "\n",
    "#--- First training of the NN ---\n",
    "loss_function, optimizer = set_learning_parameters(ae1, learning_rate=learning_rate) \n",
    "(\n",
    "    ae1,\n",
    "    training_data1,\n",
    "    X_given_z,\n",
    "    z_bins,\n",
    "    Esp_X_given_z,\n",
    "    Grad_Esp_X_given_z,\n",
    "    Std1_X_given_z,\n",
    "    Std2_X_given_z,\n",
    "    f_dec_z\n",
    ") = train_with_variance_decomposition_plots(ae1,\n",
    "                                            loss_function,\n",
    "                                            optimizer,\n",
    "                                            trajectory,\n",
    "                                            np.ones(trajectory.shape[0]),\n",
    "                                            batch_size=batch_size,\n",
    "                                            num_epochs=num_epochs,\n",
    "                                            n_bins_z=n_bins_z,\n",
    "                                            x_domain=x_domain,\n",
    "                                            y_domain=y_domain\n",
    "                              )\n",
    "\n",
    "#--- Compute average losses per epoch ---\n",
    "loss_evol1 = []\n",
    "for i in range(len(training_data1)):\n",
    "    loss_evol1.append([torch.mean(training_data1[i][0]), torch.mean(training_data1[i][1])])\n",
    "loss_evol1 = np.array(loss_evol1)\n",
    "\n",
    "#--- Compute things to do \"nice\" plots ---\n",
    "# Ort_Grad_Esp_X_given_z is a unit tangent vector to the isoline \\xi = cst\n",
    "# computed by taking the normalized orthogonal to \\nabla \\xi\n",
    "Ort_Grad_Esp_X_given_z = [[] for i in range(num_epochs)]\n",
    "# Conditional average of X plus/minus 1.96*standard deviation in the tangent direction\n",
    "Esp_p_X_given_z = [[] for i in range(num_epochs)]\n",
    "Esp_m_X_given_z = [[] for i in range(num_epochs)]\n",
    "Var_Esp_X_given_z = np.zeros(num_epochs)\n",
    "for epoch in range(num_epochs):\n",
    "    j = 0\n",
    "    # loop over the lists over bins to eliminate empty bins\n",
    "    while j < len(z_bins[epoch][0]):\n",
    "        if len(X_given_z[epoch][j]) == 0:\n",
    "            z_bins[epoch][0].pop(j)\n",
    "            Esp_X_given_z[epoch].pop(j)\n",
    "            Grad_Esp_X_given_z[epoch].pop(j)\n",
    "            Std1_X_given_z[epoch].pop(j)\n",
    "            Std2_X_given_z[epoch].pop(j) \n",
    "            X_given_z[epoch].pop(j)\n",
    "            f_dec_z[epoch].pop(j)\n",
    "        else:\n",
    "            j += 1\n",
    "    for j in range(len(z_bins[epoch][0])):\n",
    "        # computation of the tangent vector orthogonal to \\nabla \\xi = (a_1,a_2), i.e. t = (-a_2,a_1)/normalization\n",
    "        Ort_Grad_Esp_X_given_z[epoch].append(np.zeros(2))\n",
    "        Ort_Grad_Esp_X_given_z[epoch][j][0] = - Grad_Esp_X_given_z[epoch][j][1] \n",
    "        Ort_Grad_Esp_X_given_z[epoch][j][1] = Grad_Esp_X_given_z[epoch][j][0] \n",
    "        Ort_Grad_Esp_X_given_z[epoch][j] = Ort_Grad_Esp_X_given_z[epoch][j] / np.sqrt(np.sum(Grad_Esp_X_given_z[epoch][j]**2))\n",
    "        # plotting average of X in a bin +/- standard deviation along tangent vector\n",
    "        Esp_m_X_given_z[epoch].append(Esp_X_given_z[epoch][j] - 1.96 * Std1_X_given_z[epoch][j] * Ort_Grad_Esp_X_given_z[epoch][j])\n",
    "        Esp_p_X_given_z[epoch].append(Esp_X_given_z[epoch][j] + 1.96 * Std1_X_given_z[epoch][j] * Ort_Grad_Esp_X_given_z[epoch][j])\n",
    "    Var_Esp_X_given_z[epoch] = np.std(np.array(Esp_X_given_z[epoch]))\n",
    "\n",
    "# Obtain values of the RC and potential on a grid to plot contour lines\n",
    "xi_ae1_on_grid = np.zeros([100, 100])\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        x = np.array([gridx[i], gridy[j]])\n",
    "        xi_ae1_on_grid[i,j] = xi_ae(ae1, x)\n",
    "\n",
    "#--- Plot the results ---\n",
    "start_epoch_index = 1\n",
    "fig, (ax0, ax1)  = plt.subplots(1,2, figsize=(10,4)) \n",
    "ax0.plot(loss_evol1[start_epoch_index:, 0], '--', label='train loss', marker='x')\n",
    "ax0.plot(range(1, len(loss_evol1[start_epoch_index:, 1])), loss_evol1[start_epoch_index: -1, 1], '-.', label='test loss', marker='+')\n",
    "ax0.legend()\n",
    "ax1.pcolormesh(x_plot, y_plot, potential_on_grid, cmap='coolwarm_r',shading='auto')\n",
    "ax1.contour(x_plot, y_plot, xi_ae1_on_grid, 40, cmap = 'viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e90cd6",
   "metadata": {},
   "source": [
    "This one also has over fitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d52fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- variance of conditional averages ---\n",
    "plt.figure()\n",
    "plt.plot(range(num_epochs), Var_Esp_X_given_z)\n",
    "plt.title('Variance of conditional averages as a function of epochs')\n",
    "plt.xlabel('epoch number')\n",
    "## We should have something constant here as the encoder is supposed to stay the same.\n",
    "\n",
    "\n",
    "#--- Conditionnal expectancy and decoded values ---\n",
    "index = -1\n",
    "plt.figure()\n",
    "plt.title('End of last epoch')\n",
    "plt.plot(np.array(Esp_X_given_z[index])[:, 0], np.array(Esp_X_given_z[index])[:, 1], '-o', color='b', label='cond. avg.')\n",
    "plt.plot(np.array(Esp_p_X_given_z[index])[:, 0], np.array(Esp_p_X_given_z[index])[:, 1], '-.+', color='b')\n",
    "plt.plot(np.array(Esp_m_X_given_z[index])[:, 0], np.array(Esp_m_X_given_z[index])[:, 1], '-.+', color='b')\n",
    "plt.plot(np.array(f_dec_z[index])[:, 0], np.array(f_dec_z[index])[:, 1], '*', color='black', label='decoder last epoch')\n",
    "plt.pcolormesh(x_plot, y_plot, potential_on_grid, cmap='coolwarm_r',shading='auto')\n",
    "index = 0\n",
    "plt.plot(np.array(f_dec_z[index])[:, 0], np.array(f_dec_z[index])[:, 1], '*', color='pink', label='decoder first epoch')\n",
    "plt.pcolormesh(x_plot, y_plot, potential_on_grid, cmap='coolwarm_r',shading='auto')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f81aea7",
   "metadata": {},
   "source": [
    "Let's try to converge the decoder given this encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25322f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "num_epochs = 150\n",
    "learning_rate = 0.001\n",
    "n_bins_z = 20          # number of bins in the Encoded dimension\n",
    "optimizer_algo='Adam'  # Adam by default, otherwise SGD\n",
    "## ae1 = DeepAutoEncoder(2, [3, 3, 2], 1) ### We don't want  to re-initialise the model here\n",
    "print(ae1) \n",
    "\n",
    "\n",
    "#--- First training of the NN ---\n",
    "loss_function, optimizer = set_learning_parameters(ae1.decoder, learning_rate=learning_rate) ## here we only give the decoder as model to the optimizer.\n",
    "(\n",
    "    ae1,\n",
    "    training_data1,\n",
    "    X_given_z,\n",
    "    z_bins,\n",
    "    Esp_X_given_z,\n",
    "    Grad_Esp_X_given_z,\n",
    "    Std1_X_given_z,\n",
    "    Std2_X_given_z,\n",
    "    f_dec_z\n",
    ") = train_with_variance_decomposition_plots(ae1,\n",
    "                                            loss_function,\n",
    "                                            optimizer,\n",
    "                                            trajectory,\n",
    "                                            np.ones(trajectory.shape[0]),\n",
    "                                            batch_size=batch_size,\n",
    "                                            num_epochs=num_epochs,\n",
    "                                            n_bins_z=n_bins_z,\n",
    "                                            x_domain=x_domain,\n",
    "                                            y_domain=y_domain\n",
    "                              )\n",
    "\n",
    "#--- Compute average losses per epoch ---\n",
    "loss_evol1 = []\n",
    "for i in range(len(training_data1)):\n",
    "    loss_evol1.append([torch.mean(training_data1[i][0]), torch.mean(training_data1[i][1])])\n",
    "loss_evol1 = np.array(loss_evol1)\n",
    "\n",
    "#--- Compute things to do \"nice\" plots ---\n",
    "# Ort_Grad_Esp_X_given_z is a unit tangent vector to the isoline \\xi = cst\n",
    "# computed by taking the normalized orthogonal to \\nabla \\xi\n",
    "Ort_Grad_Esp_X_given_z = [[] for i in range(num_epochs)]\n",
    "# Conditional average of X plus/minus 1.96*standard deviation in the tangent direction\n",
    "Esp_p_X_given_z = [[] for i in range(num_epochs)]\n",
    "Esp_m_X_given_z = [[] for i in range(num_epochs)]\n",
    "Var_Esp_X_given_z = np.zeros(num_epochs)\n",
    "for epoch in range(num_epochs):\n",
    "    j = 0\n",
    "    # loop over the lists over bins to eliminate empty bins\n",
    "    while j < len(z_bins[epoch][0]):\n",
    "        if len(X_given_z[epoch][j]) == 0:\n",
    "            z_bins[epoch][0].pop(j)\n",
    "            Esp_X_given_z[epoch].pop(j)\n",
    "            Grad_Esp_X_given_z[epoch].pop(j)\n",
    "            Std1_X_given_z[epoch].pop(j)\n",
    "            Std2_X_given_z[epoch].pop(j) \n",
    "            X_given_z[epoch].pop(j)\n",
    "            f_dec_z[epoch].pop(j)\n",
    "        else:\n",
    "            j += 1\n",
    "    for j in range(len(z_bins[epoch][0])):\n",
    "        # computation of the tangent vector orthogonal to \\nabla \\xi = (a_1,a_2), i.e. t = (-a_2,a_1)/normalization\n",
    "        Ort_Grad_Esp_X_given_z[epoch].append(np.zeros(2))\n",
    "        Ort_Grad_Esp_X_given_z[epoch][j][0] = - Grad_Esp_X_given_z[epoch][j][1] \n",
    "        Ort_Grad_Esp_X_given_z[epoch][j][1] = Grad_Esp_X_given_z[epoch][j][0] \n",
    "        Ort_Grad_Esp_X_given_z[epoch][j] = Ort_Grad_Esp_X_given_z[epoch][j] / np.sqrt(np.sum(Grad_Esp_X_given_z[epoch][j]**2))\n",
    "        # plotting average of X in a bin +/- standard deviation along tangent vector\n",
    "        Esp_m_X_given_z[epoch].append(Esp_X_given_z[epoch][j] - 1.96 * Std1_X_given_z[epoch][j] * Ort_Grad_Esp_X_given_z[epoch][j])\n",
    "        Esp_p_X_given_z[epoch].append(Esp_X_given_z[epoch][j] + 1.96 * Std1_X_given_z[epoch][j] * Ort_Grad_Esp_X_given_z[epoch][j])\n",
    "    Var_Esp_X_given_z[epoch] = np.std(np.array(Esp_X_given_z[epoch]))\n",
    "\n",
    "# Obtain values of the RC and potential on a grid to plot contour lines\n",
    "xi_ae1_on_grid = np.zeros([100, 100])\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        x = np.array([gridx[i], gridy[j]])\n",
    "        xi_ae1_on_grid[i,j] = xi_ae(ae1, x)\n",
    "\n",
    "#--- Plot the results ---\n",
    "start_epoch_index = 1\n",
    "fig, (ax0, ax1)  = plt.subplots(1,2, figsize=(10,4)) \n",
    "ax0.plot(loss_evol1[start_epoch_index:, 0], '--', label='train loss', marker='x')\n",
    "ax0.plot(range(1, len(loss_evol1[start_epoch_index:, 1])), loss_evol1[start_epoch_index: -1, 1], '-.', label='test loss', marker='+')\n",
    "ax0.legend()\n",
    "ax1.pcolormesh(x_plot, y_plot, potential_on_grid, cmap='coolwarm_r',shading='auto')\n",
    "ax1.contour(x_plot, y_plot, xi_ae1_on_grid, 40, cmap = 'viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421eb358",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#--- variance of conditional averages ---\n",
    "plt.figure()\n",
    "plt.plot(range(num_epochs), Var_Esp_X_given_z)\n",
    "plt.title('Variance of conditional averages as a function of epochs')\n",
    "plt.xlabel('epoch number')\n",
    "## We should have something constant here as the encoder is supposed to stay the same.\n",
    "\n",
    "\n",
    "#--- Conditionnal expectancy and decoded values ---\n",
    "index = -1\n",
    "plt.figure()\n",
    "plt.title('End of last epoch')\n",
    "plt.plot(np.array(Esp_X_given_z[index])[:, 0], np.array(Esp_X_given_z[index])[:, 1], '-o', color='b', label='cond. avg.')\n",
    "plt.plot(np.array(Esp_p_X_given_z[index])[:, 0], np.array(Esp_p_X_given_z[index])[:, 1], '-.+', color='b')\n",
    "plt.plot(np.array(Esp_m_X_given_z[index])[:, 0], np.array(Esp_m_X_given_z[index])[:, 1], '-.+', color='b')\n",
    "plt.plot(np.array(f_dec_z[index])[:, 0], np.array(f_dec_z[index])[:, 1], '*', color='black', label='decoder last epoch')\n",
    "plt.pcolormesh(x_plot, y_plot, potential_on_grid, cmap='coolwarm_r',shading='auto')\n",
    "index = 0\n",
    "plt.plot(np.array(f_dec_z[index])[:, 0], np.array(f_dec_z[index])[:, 1], '*', color='pink', label='decoder first epoch')\n",
    "plt.pcolormesh(x_plot, y_plot, potential_on_grid, cmap='coolwarm_r',shading='auto')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c2f906",
   "metadata": {},
   "source": [
    "It is getting better and better but it is quite difficult still. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
